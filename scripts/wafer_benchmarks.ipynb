{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/lightly-ai/lightly/blob/master/docs/source/getting_started/benchmarks/imagenette_benchmark.py\n",
    "\n",
    "| Model         | Batch Size | Epochs |  KNN Test Accuracy |       Time | Peak GPU Usage |\n",
    "|---------------|------------|--------|--------------------|------------|----------------|\n",
    "| BarlowTwins   |        256 |    200 |              0.587 |   86.2 Min |      4.0 GByte |\n",
    "| BYOL          |        256 |    200 |              0.619 |   88.6 Min |      4.3 GByte |\n",
    "| DCL (*)       |        256 |    200 |              0.762 |   53.3 Min |      4.3 GByte |\n",
    "| DCLW (*)      |        256 |    200 |              0.755 |   53.7 Min |      4.3 GByte |\n",
    "| DINO (Res18)  |        256 |    200 |              0.736 |   86.5 Min |      4.1 GByte |\n",
    "| MSN (ViT-S)   |        256 |    200 |              0.741 |   92.7 Min |     16.3 GByte |\n",
    "| Moco          |        256 |    200 |              0.727 |   87.3 Min |      4.3 GByte |\n",
    "| NNCLR         |        256 |    200 |              0.726 |   86.8 Min |      4.2 GByte |\n",
    "| SimCLR        |        256 |    200 |              0.771 |   82.2 Min |      3.9 GByte |\n",
    "| SimSiam       |        256 |    200 |              0.669 |   78.6 Min |      3.9 GByte |\n",
    "| SMoG          |        128 |    200 |              0.698 |  220.9 Min |     14.3 GByte |\n",
    "| SwaV          |        256 |    200 |              0.748 |   77.6 Min |      4.0 GByte |\n",
    "|---------------|------------|--------|--------------------|------------|----------------|\n",
    "| BarlowTwins   |        256 |    800 |              0.789 |  330.9 Min |      4.0 GByte |\n",
    "| BYOL          |        256 |    800 |              0.851 |  332.7 Min |      4.3 GByte |\n",
    "| DCL (*)       |        256 |    800 |              0.816 |  213.1 Min |      4.3 GByte |\n",
    "| DCLW (*)      |        256 |    800 |              0.827 |  213.1 Min |      4.3 GByte |\n",
    "| DINO (Res18)  |        256 |    800 |              0.881 |  613.9 Min |      6.7 GByte |\n",
    "| MSN (ViT-S)   |        256 |    800 |              0.834 |  376.1 Min |     16.3 GByte |\n",
    "| Moco          |        256 |    800 |              0.832 |  322.8 Min |      4.2 GByte |\n",
    "| NNCLR         |        256 |    800 |              0.848 |  341.4 Min |      4.2 GByte |\n",
    "| SimCLR        |        256 |    800 |              0.858 |  324.8 Min |      3.9 GByte |\n",
    "| SimSiam       |        256 |    800 |              0.852 |  316.0 Min |      3.9 GByte |\n",
    "| SwaV          |        256 |    800 |              0.899 |  554.7 Min |      6.6 GByte |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models._registry import register_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\khanm/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "from torch.hub import load\n",
    "backbone = load(\"facebookresearch/dino:main\", \"dino_vits16\", pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import lightly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.data.collate import MultiViewCollateFunction, SimCLRCollateFunction\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.models import modules, utils\n",
    "from lightly.models.modules import heads, masked_autoencoder\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import (\n",
    "    Accuracy,\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassAUROC,\n",
    "    MulticlassF1Score,\n",
    ")\n",
    "from torchmetrics.functional.classification import (\n",
    "    multiclass_accuracy,\n",
    "    multiclass_f1_score,\n",
    ")\n",
    "from utilities.benchmarking import KNNBenchmarkModule\n",
    "from utilities.data import *\n",
    "\n",
    "# suppress annoying torchmetrics and lightning warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*interpolation.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*meaningless.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*log_every_n_steps.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_root_dir = os.path.join(os.getcwd(), \"benchmark_logs\")\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "memory_bank_size = 4096\n",
    "\n",
    "# set max_epochs to 800 for long run (takes around 10h on a single V100)\n",
    "max_epochs = 5\n",
    "knn_k = 200\n",
    "knn_t = 0.1\n",
    "classes = 9\n",
    "input_size = 200\n",
    "\n",
    "#  Set to True to enable Distributed Data Parallel training.\n",
    "distributed = False\n",
    "\n",
    "# Set to True to enable Synchronized Batch Norm (requires distributed=True).\n",
    "# If enabled the batch norm is calculated over all gpus, otherwise the batch\n",
    "# norm is only calculated from samples on the same gpu.\n",
    "sync_batchnorm = False\n",
    "\n",
    "# Set to True to gather features from all gpus before calculating\n",
    "# the loss (requires distributed=True).\n",
    "#  If enabled then the loss on every gpu is calculated with features from all\n",
    "# gpus, otherwise only features from the same gpu are used.\n",
    "gather_distributed = False\n",
    "\n",
    "# benchmark\n",
    "n_runs = 3  # optional, increase to create multiple runs and report mean + std\n",
    "batch_size = 32\n",
    "lr_factor = batch_size / 256  #  scales the learning rate linearly with batch size\n",
    "\n",
    "# use a GPU if available\n",
    "gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "\n",
    "if distributed:\n",
    "    distributed_backend = \"ddp\"\n",
    "    # reduce batch size for distributed training\n",
    "    batch_size = batch_size // gpus\n",
    "else:\n",
    "    distributed_backend = None\n",
    "    # limit to single gpu if not using distributed training\n",
    "    gpus = min(gpus, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/cleaned_splits/train_1_split.pkl\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.waferMap, df.failureCode, test_size=0.25, random_state=42)\n",
    "\n",
    "dataset_train_ssl = LightlyDataset.from_torch_dataset(\n",
    "    WaferMapDataset(X_train, y_train)\n",
    ")\n",
    "\n",
    "# we use test transformations for getting the feature for kNN on train data\n",
    "dataset_train_kNN = LightlyDataset.from_torch_dataset(\n",
    "    WaferMapDataset(X_train, y_train), transform=get_inference_transforms()\n",
    ")\n",
    "\n",
    "dataset_test = LightlyDataset.from_torch_dataset(\n",
    "    WaferMapDataset(X_val, y_val), transform=get_inference_transforms()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base collate function for basic joint embedding\n",
    "# e.g. SimCLR, MoCo, BYOL, Barlow Twins, or SimSiam\n",
    "collate_fn = WaferImageCollateFunction([input_size, input_size])\n",
    "\n",
    "# Multi crop augmentation for DINO\n",
    "dino_collate_fn = WaferDINOCOllateFunction(\n",
    "    global_crop_size=input_size, local_crop_size=input_size // 2\n",
    ")\n",
    "\n",
    "fastsiam_collate_fn = WaferFastSiamCollateFunction([input_size, input_size])\n",
    "\n",
    "# Multi crop augmentation for MSN\n",
    "msn_collate_fn = WaferMSNCollateFunction(\n",
    "    random_size=input_size, focal_size=input_size // 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightly.utils.debug import plot_augmented_images, std_of_l2_normalized\n",
    "\n",
    "# plot_augmented_images(\n",
    "#     # Grab 5 random samples from X_train to visualize original and augmented images for each\n",
    "#     X_train.sample(5).tolist(),\n",
    "#     collate_fn,\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Using cache found in C:\\Users\\khanm/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'trunc_normal_' from 'utils' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 846\u001b[0m\n\u001b[0;32m    841\u001b[0m pl\u001b[39m.\u001b[39mseed_everything(seed)\n\u001b[0;32m    842\u001b[0m dataloader_train_ssl, dataloader_train_kNN, dataloader_test \u001b[39m=\u001b[39m get_data_loaders(\n\u001b[0;32m    843\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    844\u001b[0m     model\u001b[39m=\u001b[39mBenchmarkModel,\n\u001b[0;32m    845\u001b[0m )\n\u001b[1;32m--> 846\u001b[0m benchmark_model \u001b[39m=\u001b[39m BenchmarkModel(dataloader_train_kNN, classes)\n\u001b[0;32m    848\u001b[0m \u001b[39m# Save logs to: {CWD}/benchmark_logs/cifar10/{experiment_version}/{model_name}/\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39m# If multiple runs are specified a subdirectory for each run is created.\u001b[39;00m\n\u001b[0;32m    850\u001b[0m sub_dir \u001b[39m=\u001b[39m model_name \u001b[39mif\u001b[39;00m n_runs \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m/run\u001b[39m\u001b[39m{\u001b[39;00mseed\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 520\u001b[0m, in \u001b[0;36mDINOViTModel.__init__\u001b[1;34m(self, dataloader_kNN, num_classes)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(dataloader_kNN, num_classes)\n\u001b[0;32m    516\u001b[0m \u001b[39m# self.backbone = timm.create_model(\"vit_tiny_patch16_224\", num_classes=0)\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[39m# feature_dim = (\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[39m#     timm.create_model(\"vit_tiny_patch16_224\").get_classifier().in_features\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m backbone \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mhub\u001b[39m.\u001b[39;49mload(\n\u001b[0;32m    521\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mfacebookresearch/dino:main\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdino_vits16\u001b[39;49m\u001b[39m\"\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    522\u001b[0m )\n\u001b[0;32m    523\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone \u001b[39m=\u001b[39m backbone\n\u001b[0;32m    524\u001b[0m feature_dim \u001b[39m=\u001b[39m backbone\u001b[39m.\u001b[39membed_dim\n",
      "File \u001b[1;32mc:\\Users\\khanm\\anaconda3\\envs\\timm-dev\\lib\\site-packages\\torch\\hub.py:542\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m source \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    539\u001b[0m     repo_or_dir \u001b[39m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[39m\"\u001b[39m\u001b[39mload\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    540\u001b[0m                                        verbose\u001b[39m=\u001b[39mverbose, skip_validation\u001b[39m=\u001b[39mskip_validation)\n\u001b[1;32m--> 542\u001b[0m model \u001b[39m=\u001b[39m _load_local(repo_or_dir, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    543\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\khanm\\anaconda3\\envs\\timm-dev\\lib\\site-packages\\torch\\hub.py:569\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, hubconf_dir)\n\u001b[0;32m    568\u001b[0m hubconf_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[1;32m--> 569\u001b[0m hub_module \u001b[39m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    571\u001b[0m entry \u001b[39m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m    572\u001b[0m model \u001b[39m=\u001b[39m entry(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\khanm\\anaconda3\\envs\\timm-dev\\lib\\site-packages\\torch\\hub.py:90\u001b[0m, in \u001b[0;36m_import_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m     88\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mmodule_from_spec(spec)\n\u001b[0;32m     89\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(spec\u001b[39m.\u001b[39mloader, Loader)\n\u001b[1;32m---> 90\u001b[0m spec\u001b[39m.\u001b[39;49mloader\u001b[39m.\u001b[39;49mexec_module(module)\n\u001b[0;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m module\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_dino_main\\hubconf.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresnet\u001b[39;00m \u001b[39mimport\u001b[39;00m resnet50\n\u001b[1;32m---> 17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mvision_transformer\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mvits\u001b[39;00m\n\u001b[0;32m     19\u001b[0m dependencies \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorchvision\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdino_vits16\u001b[39m(pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_dino_main\\vision_transformer.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m trunc_normal_\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop_path\u001b[39m(x, drop_prob: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m, training: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m drop_prob \u001b[39m==\u001b[39m \u001b[39m0.\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m training:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'trunc_normal_' from 'utils' (unknown location)"
     ]
    }
   ],
   "source": [
    "def get_data_loaders(batch_size: int, model):\n",
    "    \"\"\"Helper method to create dataloaders for ssl, kNN train and kNN test\n",
    "\n",
    "    Args:\n",
    "        batch_size: Desired batch size for all dataloaders\n",
    "    \"\"\"\n",
    "    col_fn = collate_fn\n",
    "    # if the model is any of the DINO models, we use the DINO collate function\n",
    "    if (\n",
    "        model == DINOModel\n",
    "        or model == DINOConvNeXtModel\n",
    "        or model == DINOXCiTModel\n",
    "        or model == DINOViTModel\n",
    "        or model == DINOSWINModel\n",
    "    ):\n",
    "        col_fn = dino_collate_fn\n",
    "    elif model == MSNModel:\n",
    "        col_fn = msn_collate_fn\n",
    "    elif model == FastSiamModel:\n",
    "        col_fn = fastsiam_collate_fn\n",
    "\n",
    "    dataloader_train_ssl = DataLoader(\n",
    "        dataset_train_ssl,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=col_fn,\n",
    "        drop_last=True,\n",
    "        # num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dataloader_train_kNN = DataLoader(\n",
    "        dataset_train_kNN,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        # num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dataloader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        # num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader_train_ssl, dataloader_train_kNN, dataloader_test\n",
    "\n",
    "\n",
    "class MocoModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        num_splits = 0 if sync_batchnorm else 8\n",
    "        # # TODO: Add split batch norm to the resnet model\n",
    "        # resnet = torchvision.models.resnet18()\n",
    "        # feature_dim = list(resnet.children())[-1].in_features\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        # )\n",
    "\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "\n",
    "        # create a moco model based on ResNet\n",
    "        self.projection_head = heads.MoCoProjectionHead(feature_dim, 2048, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # create our loss with the optional memory bank\n",
    "        self.criterion = lightly.loss.NTXentLoss(\n",
    "            temperature=0.1, memory_bank_size=memory_bank_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        return self.projection_head(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "\n",
    "        # update momentum\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "        utils.update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "        def step(x0_, x1_):\n",
    "            x1_, shuffle = utils.batch_shuffle(x1_, distributed=distributed)\n",
    "            x0_ = self.backbone(x0_).flatten(start_dim=1)\n",
    "            x0_ = self.projection_head(x0_)\n",
    "\n",
    "            x1_ = self.backbone_momentum(x1_).flatten(start_dim=1)\n",
    "            x1_ = self.projection_head_momentum(x1_)\n",
    "            x1_ = utils.batch_unshuffle(x1_, shuffle, distributed=distributed)\n",
    "            return x0_, x1_\n",
    "\n",
    "        # We use a symmetric loss (model trains faster at little compute overhead)\n",
    "        # https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb\n",
    "        loss_1 = self.criterion(*step(x0, x1))\n",
    "        loss_2 = self.criterion(*step(x1, x0))\n",
    "\n",
    "        loss = 0.5 * (loss_1 + loss_2)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.backbone.parameters()) + list(\n",
    "            self.projection_head.parameters()\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class SimCLRModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        # resnet = torchvision.models.resnet18()\n",
    "        # feature_dim = list(resnet.children())[-1].in_features\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        # )\n",
    "\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "        self.projection_head = heads.SimCLRProjectionHead(feature_dim, feature_dim, 128)\n",
    "        self.criterion = lightly.loss.NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class SimSiamModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        # resnet = torchvision.models.resnet18()\n",
    "        # feature_dim = list(resnet.children())[-1].in_features\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        # )\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "        self.projection_head = heads.SimSiamProjectionHead(feature_dim, 2048, 2048)\n",
    "        self.prediction_head = heads.SimSiamPredictionHead(2048, 512, 2048)\n",
    "        self.criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(f)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "        loss = 0.5 * (self.criterion(z0, p1) + self.criterion(z1, p0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2,  #  no lr-scaling, results in better training stability\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class FastSiamModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "        self.projection_head = heads.SimSiamProjectionHead(feature_dim, 2048, 2048)\n",
    "        self.prediction_head = heads.SimSiamPredictionHead(2048, 512, 2048)\n",
    "        self.criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(f)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Unpack augmented views\n",
    "        views, _, _ = batch\n",
    "        x1, x2, x3, x4 = views\n",
    "\n",
    "        # Pass each view through projector to get z, and predictor to get p\n",
    "        z1, p1 = self.forward(x1)\n",
    "        z2, p2 = self.forward(x2)\n",
    "        z3, p3 = self.forward(x3)\n",
    "        z4, p4 = self.forward(x4)\n",
    "\n",
    "        # Use mean of the last N - 1 projected views\n",
    "        mean = (z2 + z3 + z4) / 3\n",
    "\n",
    "        # Compute loss using prediction of 1st view, mean of remaining projected views\n",
    "        loss = self.criterion(p1, mean)\n",
    "\n",
    "        # Keep a log of the loss\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2,  #  no lr-scaling, results in better training stability\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class BarlowTwinsModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        # resnet = torchvision.models.resnet18()\n",
    "        # feature_dim = list(resnet.children())[-1].in_features\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        # )\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "        # use a 2-layer projection head for cifar10 as described in the paper\n",
    "        self.projection_head = heads.BarlowTwinsProjectionHead(feature_dim, 2048, 2048)\n",
    "\n",
    "        self.criterion = lightly.loss.BarlowTwinsLoss(\n",
    "            gather_distributed=gather_distributed\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class BYOLModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        # resnet = torchvision.models.resnet18()\n",
    "        # feature_dim = list(resnet.children())[-1].in_features\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        # )\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "\n",
    "        # create a byol model based on ResNet\n",
    "        self.projection_head = heads.BYOLProjectionHead(feature_dim, 4096, 256)\n",
    "        self.prediction_head = heads.BYOLPredictionHead(256, 4096, 256)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        self.criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        return p\n",
    "\n",
    "    def forward_momentum(self, x):\n",
    "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y)\n",
    "        z = z.detach()\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, m=0.99)\n",
    "        utils.update_momentum(\n",
    "            self.projection_head, self.projection_head_momentum, m=0.99\n",
    "        )\n",
    "        (x0, x1), _, _ = batch\n",
    "        p0 = self.forward(x0)\n",
    "        z0 = self.forward_momentum(x0)\n",
    "        p1 = self.forward(x1)\n",
    "        z1 = self.forward_momentum(x1)\n",
    "        loss = 0.5 * (self.criterion(p0, z1) + self.criterion(p1, z0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = (\n",
    "            list(self.backbone.parameters())\n",
    "            + list(self.projection_head.parameters())\n",
    "            + list(self.prediction_head.parameters())\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOConvNeXtModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        self.backbone = timm.create_model(\"convnextv2_nano\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"convnextv2_nano\").get_classifier().in_features\n",
    "\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOXCiTModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        self.backbone = timm.create_model(\"xcit_tiny_12_p16_224\", num_classes=0)\n",
    "        feature_dim = (\n",
    "            timm.create_model(\"xcit_tiny_12_p16_224\").get_classifier().in_features\n",
    "        )\n",
    "\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOViTModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # self.backbone = timm.create_model(\"vit_tiny_patch16_224\", num_classes=0)\n",
    "        # feature_dim = (\n",
    "        #     timm.create_model(\"vit_tiny_patch16_224\").get_classifier().in_features\n",
    "        # )\n",
    "        self.backbone = backbone\n",
    "        feature_dim = backbone.embed_dim\n",
    "\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOSWINModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        self.backbone = timm.create_model(\"swinv2_tiny_window16_256\", num_classes=0)\n",
    "        feature_dim = (\n",
    "            timm.create_model(\"swinv2_tiny_window16_256\").get_classifier().in_features\n",
    "        )\n",
    "\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class MAEModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        decoder_dim = 512\n",
    "        vit = torchvision.models.vit_b_32(pretrained=False)\n",
    "\n",
    "        self.warmup_epochs = 40 if max_epochs >= 800 else 20\n",
    "        self.mask_ratio = 0.75\n",
    "        self.patch_size = vit.patch_size\n",
    "        self.sequence_length = vit.seq_length\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_dim))\n",
    "        self.backbone = masked_autoencoder.MAEBackbone.from_vit(vit)\n",
    "        self.decoder = masked_autoencoder.MAEDecoder(\n",
    "            seq_length=vit.seq_length,\n",
    "            num_layers=1,\n",
    "            num_heads=16,\n",
    "            embed_input_dim=vit.hidden_dim,\n",
    "            hidden_dim=decoder_dim,\n",
    "            mlp_dim=decoder_dim * 4,\n",
    "            out_dim=vit.patch_size**2 * 3,\n",
    "            dropout=0,\n",
    "            attention_dropout=0,\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward_encoder(self, images, idx_keep=None):\n",
    "        return self.backbone.encode(images, idx_keep)\n",
    "\n",
    "    def forward_decoder(self, x_encoded, idx_keep, idx_mask):\n",
    "        # build decoder input\n",
    "        batch_size = x_encoded.shape[0]\n",
    "        x_decode = self.decoder.embed(x_encoded)\n",
    "        x_masked = utils.repeat_token(\n",
    "            self.mask_token, (batch_size, self.sequence_length)\n",
    "        )\n",
    "        x_masked = utils.set_at_index(x_masked, idx_keep, x_decode)\n",
    "\n",
    "        # decoder forward pass\n",
    "        x_decoded = self.decoder.decode(x_masked)\n",
    "\n",
    "        # predict pixel values for masked tokens\n",
    "        x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
    "        x_pred = self.decoder.predict(x_pred)\n",
    "        return x_pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, _, _ = batch\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "        idx_keep, idx_mask = utils.random_token_mask(\n",
    "            size=(batch_size, self.sequence_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=images.device,\n",
    "        )\n",
    "        x_encoded = self.forward_encoder(images, idx_keep)\n",
    "        x_pred = self.forward_decoder(x_encoded, idx_keep, idx_mask)\n",
    "\n",
    "        # get image patches for masked tokens\n",
    "        patches = utils.patchify(images, self.patch_size)\n",
    "        # must adjust idx_mask for missing class token\n",
    "        target = utils.get_at_index(patches, idx_mask - 1)\n",
    "\n",
    "        loss = self.criterion(x_pred, target)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=1.5e-4 * lr_factor,\n",
    "            weight_decay=0.05,\n",
    "            betas=(0.9, 0.95),\n",
    "        )\n",
    "        cosine_with_warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optim, self.scale_lr\n",
    "        )\n",
    "        return [optim], [cosine_with_warmup_scheduler]\n",
    "\n",
    "    def scale_lr(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return epoch / self.warmup_epochs\n",
    "        else:\n",
    "            return 0.5 * (\n",
    "                1.0\n",
    "                + math.cos(\n",
    "                    math.pi\n",
    "                    * (epoch - self.warmup_epochs)\n",
    "                    / (max_epochs - self.warmup_epochs)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "class MSNModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        self.warmup_epochs = 15\n",
    "        #  ViT small configuration (ViT-S/16) = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)\n",
    "        #  ViT tiny configuration (ViT-T/16) = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)\n",
    "        self.mask_ratio = 0.15\n",
    "        self.backbone = masked_autoencoder.MAEBackbone(\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            num_layers=12,\n",
    "            num_heads=6,\n",
    "            hidden_dim=384,\n",
    "            mlp_dim=384 * 4,\n",
    "        )\n",
    "        self.projection_head = heads.MSNProjectionHead(384)\n",
    "\n",
    "        self.anchor_backbone = copy.deepcopy(self.backbone)\n",
    "        self.anchor_projection_head = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone)\n",
    "        utils.deactivate_requires_grad(self.projection_head)\n",
    "\n",
    "        self.prototypes = nn.Linear(256, 1024, bias=False).weight\n",
    "        self.criterion = lightly.loss.MSNLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.anchor_backbone, self.backbone, 0.996)\n",
    "        utils.update_momentum(self.anchor_projection_head, self.projection_head, 0.996)\n",
    "\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device, non_blocking=True) for view in views]\n",
    "        targets = views[0]\n",
    "        anchors = views[1]\n",
    "        anchors_focal = torch.concat(views[2:], dim=0)\n",
    "\n",
    "        targets_out = self.backbone(targets)\n",
    "        targets_out = self.projection_head(targets_out)\n",
    "        anchors_out = self.encode_masked(anchors)\n",
    "        anchors_focal_out = self.encode_masked(anchors_focal)\n",
    "        anchors_out = torch.cat([anchors_out, anchors_focal_out], dim=0)\n",
    "\n",
    "        loss = self.criterion(anchors_out, targets_out, self.prototypes.data)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def encode_masked(self, anchors):\n",
    "        batch_size, _, _, width = anchors.shape\n",
    "        seq_length = (width // self.anchor_backbone.patch_size) ** 2\n",
    "        idx_keep, _ = utils.random_token_mask(\n",
    "            size=(batch_size, seq_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=self.device,\n",
    "        )\n",
    "        out = self.anchor_backbone(anchors, idx_keep)\n",
    "        return self.anchor_projection_head(out)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = [\n",
    "            *list(self.anchor_backbone.parameters()),\n",
    "            *list(self.anchor_projection_head.parameters()),\n",
    "            self.prototypes,\n",
    "        ]\n",
    "        optim = torch.optim.AdamW(\n",
    "            params=params,\n",
    "            lr=1.5e-4 * lr_factor,\n",
    "            weight_decay=0.05,\n",
    "            betas=(0.9, 0.95),\n",
    "        )\n",
    "        cosine_with_warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optim, self.scale_lr\n",
    "        )\n",
    "        return [optim], [cosine_with_warmup_scheduler]\n",
    "\n",
    "    def scale_lr(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return epoch / self.warmup_epochs\n",
    "        else:\n",
    "            return 0.5 * (\n",
    "                1.0\n",
    "                + math.cos(\n",
    "                    math.pi\n",
    "                    * (epoch - self.warmup_epochs)\n",
    "                    / (max_epochs - self.warmup_epochs)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "models = [\n",
    "    # MSNModel,  # disabled by default because MSN uses larger images with size 224\n",
    "    # DINOModel,\n",
    "    DINOViTModel,\n",
    "    DINOConvNeXtModel,\n",
    "    DINOXCiTModel,\n",
    "    DINOSWINModel,\n",
    "    # FastSiamModel,\n",
    "    # SimSiamModel,\n",
    "    # SimCLRModel,\n",
    "    # MocoModel,\n",
    "    # BarlowTwinsModel,\n",
    "    # BYOLModel,\n",
    "    # DCL,\n",
    "    # DCLW,\n",
    "    # # MAEModel, # disabled by default because MAE uses larger images with size 224\n",
    "    # MSNModel\n",
    "    # NNCLRModel,\n",
    "    # SwaVModel,\n",
    "    # SMoGModel\n",
    "]\n",
    "bench_results = dict()\n",
    "\n",
    "experiment_version = None\n",
    "# loop through configurations and train models\n",
    "for BenchmarkModel in models:\n",
    "    runs = []\n",
    "    model_name = BenchmarkModel.__name__.replace(\"Model\", \"\")\n",
    "    for seed in range(n_runs):\n",
    "        pl.seed_everything(seed)\n",
    "        dataloader_train_ssl, dataloader_train_kNN, dataloader_test = get_data_loaders(\n",
    "            batch_size=batch_size,\n",
    "            model=BenchmarkModel,\n",
    "        )\n",
    "        benchmark_model = BenchmarkModel(dataloader_train_kNN, classes)\n",
    "\n",
    "        # Save logs to: {CWD}/benchmark_logs/cifar10/{experiment_version}/{model_name}/\n",
    "        # If multiple runs are specified a subdirectory for each run is created.\n",
    "        sub_dir = model_name if n_runs <= 1 else f\"{model_name}/run{seed}\"\n",
    "        logger = TensorBoardLogger(\n",
    "            save_dir=os.path.join(logs_root_dir, \"wafermaps\"),\n",
    "            name=\"\",\n",
    "            sub_dir=sub_dir,\n",
    "            version=experiment_version,\n",
    "        )\n",
    "        if experiment_version is None:\n",
    "            # Save results of all models under same version directory\n",
    "            experiment_version = logger.version\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=os.path.join(logger.log_dir, \"checkpoints\")\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            accelerator=\"gpu\",\n",
    "            default_root_dir=logs_root_dir,\n",
    "            strategy=distributed_backend,\n",
    "            sync_batchnorm=sync_batchnorm,\n",
    "            logger=logger,\n",
    "            callbacks=[checkpoint_callback, RichProgressBar()],\n",
    "            enable_progress_bar=True,\n",
    "        )\n",
    "        start = time.time()\n",
    "        trainer.fit(\n",
    "            benchmark_model,\n",
    "            train_dataloaders=dataloader_train_ssl,\n",
    "            val_dataloaders=dataloader_test,\n",
    "        )\n",
    "        end = time.time()\n",
    "        run = {\n",
    "            \"model\": model_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": max_epochs,\n",
    "            \"max_accuracy\": benchmark_model.max_accuracy,\n",
    "            \"max_f1\": benchmark_model.max_f1,\n",
    "            \"runtime\": end - start,\n",
    "            \"gpu_memory_usage\": torch.cuda.max_memory_allocated(),\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "        runs.append(run)\n",
    "        print(run)\n",
    "\n",
    "        # delete model and trainer + free up cuda memory\n",
    "        del benchmark_model\n",
    "        del trainer\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bench_results[model_name] = runs\n",
    "\n",
    "#  print results table\n",
    "header = (\n",
    "    f\"| {'Model':<13} | {'Batch Size':>10} | {'Epochs':>6} \"\n",
    "    f\"| {'KNN Test Accuracy':>18} | {'KNN Test F1':>18} | {'Time':>10} | {'Peak GPU Usage':>14} |\"\n",
    ")\n",
    "print(\"-\" * len(header))\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for model, results in bench_results.items():\n",
    "    runtime = np.array([result[\"runtime\"] for result in results])\n",
    "    runtime = runtime.mean() / 60  # convert to min\n",
    "    accuracy = np.array([result[\"max_accuracy\"] for result in results])\n",
    "    f1 = np.array([result[\"max_f1\"] for result in results])\n",
    "    gpu_memory_usage = np.array([result[\"gpu_memory_usage\"] for result in results])\n",
    "    gpu_memory_usage = gpu_memory_usage.max() / (1024**3)  #  convert to gbyte\n",
    "\n",
    "    if len(accuracy) > 1:\n",
    "        accuracy_msg = f\"{accuracy.mean():>8.3f} +- {accuracy.std():>4.3f}\"\n",
    "    else:\n",
    "        accuracy_msg = f\"{accuracy.mean():>18.3f}\"\n",
    "    if len(f1) > 1:\n",
    "        f1_msg = f\"{f1.mean():>8.3f} +- {f1.std():>4.3f}\"\n",
    "    else:\n",
    "        f1_msg = f\"{f1.mean():>18.3f}\"\n",
    "\n",
    "    print(\n",
    "        f\"| {model:<13} | {batch_size:>10} | {max_epochs:>6} \"\n",
    "        f\"| {accuracy_msg} | {f1_msg} | {runtime:>6.1f} Min \"\n",
    "        f\"| {gpu_memory_usage:>8.1f} GByte |\",\n",
    "        flush=True,\n",
    "    )\n",
    "print(\"-\" * len(header))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random notes:\n",
    "\n",
    "**EDIT**: Now each epoch is about a minute long for FastSiam because of the changes to DieNoise. Times below are ALL out of date.\n",
    "\n",
    "| Model              | Time per Epoch |\n",
    "| -------------------| ------------------|\n",
    "| FastSiamModel      | 20 min            |\n",
    "| SimSiamModel       | 10 min            |\n",
    "| SimCLRModel        | 12 min|\n",
    "| MocoModel          | 12 min |\n",
    "| DINOModel          |35 min |\n",
    "| BarlowTwinsModel   |                    |\n",
    "| BYOLModel          |                    |\n",
    "| DCL                |                    |\n",
    "| DCLW               |                    |\n",
    "| DINOModel          |                    |\n",
    "| MAEModel           |                    |\n",
    "| MSNModel (ViT-S)  |60 min|\n",
    "| MSNModel (ViT-T)  |50 min|\n",
    "| NNCLRModel         |                    |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51c7e371e7f0e705fbface39e2a6c77c02ee1bbdcc1060cdfeb54e5726145823"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
