{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/lightly-ai/lightly/blob/master/docs/source/getting_started/benchmarks/imagenette_benchmark.py\n",
    "\n",
    "| Model         | Batch Size | Epochs |  KNN Test Accuracy |       Time | Peak GPU Usage |\n",
    "|---------------|------------|--------|--------------------|------------|----------------|\n",
    "| BarlowTwins   |        256 |    200 |              0.587 |   86.2 Min |      4.0 GByte |\n",
    "| BYOL          |        256 |    200 |              0.619 |   88.6 Min |      4.3 GByte |\n",
    "| DCL (*)       |        256 |    200 |              0.762 |   53.3 Min |      4.3 GByte |\n",
    "| DCLW (*)      |        256 |    200 |              0.755 |   53.7 Min |      4.3 GByte |\n",
    "| DINO (Res18)  |        256 |    200 |              0.736 |   86.5 Min |      4.1 GByte |\n",
    "| MSN (ViT-S)   |        256 |    200 |              0.741 |   92.7 Min |     16.3 GByte |\n",
    "| Moco          |        256 |    200 |              0.727 |   87.3 Min |      4.3 GByte |\n",
    "| NNCLR         |        256 |    200 |              0.726 |   86.8 Min |      4.2 GByte |\n",
    "| SimCLR        |        256 |    200 |              0.771 |   82.2 Min |      3.9 GByte |\n",
    "| SimSiam       |        256 |    200 |              0.669 |   78.6 Min |      3.9 GByte |\n",
    "| SMoG          |        128 |    200 |              0.698 |  220.9 Min |     14.3 GByte |\n",
    "| SwaV          |        256 |    200 |              0.748 |   77.6 Min |      4.0 GByte |\n",
    "|---------------|------------|--------|--------------------|------------|----------------|\n",
    "| BarlowTwins   |        256 |    800 |              0.789 |  330.9 Min |      4.0 GByte |\n",
    "| BYOL          |        256 |    800 |              0.851 |  332.7 Min |      4.3 GByte |\n",
    "| DCL (*)       |        256 |    800 |              0.816 |  213.1 Min |      4.3 GByte |\n",
    "| DCLW (*)      |        256 |    800 |              0.827 |  213.1 Min |      4.3 GByte |\n",
    "| DINO (Res18)  |        256 |    800 |              0.881 |  613.9 Min |      6.7 GByte |\n",
    "| MSN (ViT-S)   |        256 |    800 |              0.834 |  376.1 Min |     16.3 GByte |\n",
    "| Moco          |        256 |    800 |              0.832 |  322.8 Min |      4.2 GByte |\n",
    "| NNCLR         |        256 |    800 |              0.848 |  341.4 Min |      4.2 GByte |\n",
    "| SimCLR        |        256 |    800 |              0.858 |  324.8 Min |      3.9 GByte |\n",
    "| SimSiam       |        256 |    800 |              0.852 |  316.0 Min |      3.9 GByte |\n",
    "| SwaV          |        256 |    800 |              0.899 |  554.7 Min |      6.6 GByte |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models._registry import register_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\khanm/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "from torch.hub import load\n",
    "backbone = load(\"facebookresearch/dino:main\", \"dino_vits16\", pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import lightly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.data.collate import MultiViewCollateFunction, SimCLRCollateFunction\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.models import modules, utils\n",
    "from lightly.models.modules import heads, masked_autoencoder\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import (\n",
    "    Accuracy,\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassAUROC,\n",
    "    MulticlassF1Score,\n",
    ")\n",
    "from torchmetrics.functional.classification import (\n",
    "    multiclass_accuracy,\n",
    "    multiclass_f1_score,\n",
    ")\n",
    "from utilities.benchmarking import KNNBenchmarkModule\n",
    "from utilities.data import *\n",
    "\n",
    "# suppress annoying torchmetrics and lightning warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*interpolation.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*meaningless.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*log_every_n_steps.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_root_dir = os.path.join(os.getcwd(), \"benchmark_logs\")\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "memory_bank_size = 4096\n",
    "\n",
    "# set max_epochs to 800 for long run (takes around 10h on a single V100)\n",
    "max_epochs = 5\n",
    "knn_k = 200\n",
    "knn_t = 0.1\n",
    "classes = 9\n",
    "input_size = 200\n",
    "\n",
    "#  Set to True to enable Distributed Data Parallel training.\n",
    "distributed = False\n",
    "\n",
    "# Set to True to enable Synchronized Batch Norm (requires distributed=True).\n",
    "# If enabled the batch norm is calculated over all gpus, otherwise the batch\n",
    "# norm is only calculated from samples on the same gpu.\n",
    "sync_batchnorm = False\n",
    "\n",
    "# Set to True to gather features from all gpus before calculating\n",
    "# the loss (requires distributed=True).\n",
    "#  If enabled then the loss on every gpu is calculated with features from all\n",
    "# gpus, otherwise only features from the same gpu are used.\n",
    "gather_distributed = False\n",
    "\n",
    "# benchmark\n",
    "n_runs = 3  # optional, increase to create multiple runs and report mean + std\n",
    "batch_size = 32\n",
    "lr_factor = batch_size / 256  #  scales the learning rate linearly with batch size\n",
    "\n",
    "# use a GPU if available\n",
    "gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "\n",
    "if distributed:\n",
    "    distributed_backend = \"ddp\"\n",
    "    # reduce batch size for distributed training\n",
    "    batch_size = batch_size // gpus\n",
    "else:\n",
    "    distributed_backend = None\n",
    "    # limit to single gpu if not using distributed training\n",
    "    gpus = min(gpus, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/cleaned_splits/train_29_split.pkl\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.waferMap, df.failureCode, test_size=0.25, random_state=42)\n",
    "\n",
    "dataset_train_ssl = LightlyDataset.from_torch_dataset(\n",
    "    WaferMapDataset(X_train, y_train)\n",
    ")\n",
    "\n",
    "# we use test transformations for getting the feature for kNN on train data\n",
    "dataset_train_kNN = LightlyDataset.from_torch_dataset(\n",
    "    WaferMapDataset(X_train, y_train), transform=get_inference_transforms()\n",
    ")\n",
    "\n",
    "dataset_test = LightlyDataset.from_torch_dataset(\n",
    "    WaferMapDataset(X_val, y_val), transform=get_inference_transforms()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8007</td>\n",
       "      <td>0.591446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2112</td>\n",
       "      <td>0.156005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1099</td>\n",
       "      <td>0.081179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917</td>\n",
       "      <td>0.067735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>791</td>\n",
       "      <td>0.058428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>271</td>\n",
       "      <td>0.020018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>193</td>\n",
       "      <td>0.014256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>0.008568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>0.002364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  fraction\n",
       "8   8007  0.591446\n",
       "3   2112  0.156005\n",
       "2   1099  0.081179\n",
       "0    917  0.067735\n",
       "4    791  0.058428\n",
       "7    271  0.020018\n",
       "6    193  0.014256\n",
       "1    116  0.008568\n",
       "5     32  0.002364"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2644</td>\n",
       "      <td>0.585863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>695</td>\n",
       "      <td>0.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>406</td>\n",
       "      <td>0.089962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328</td>\n",
       "      <td>0.072679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>251</td>\n",
       "      <td>0.055617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75</td>\n",
       "      <td>0.016619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>58</td>\n",
       "      <td>0.012852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>0.009971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.002437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  fraction\n",
       "8   2644  0.585863\n",
       "3    695  0.154000\n",
       "2    406  0.089962\n",
       "0    328  0.072679\n",
       "4    251  0.055617\n",
       "7     75  0.016619\n",
       "6     58  0.012852\n",
       "1     45  0.009971\n",
       "5     11  0.002437"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_breakdown(series, title=None):\n",
    "    \"\"\"\n",
    "    Given a pandas Series, displays value_counts(), both raw counts and normalized.\n",
    "    Also plots a horizontal bar chart of normalized value counts.\n",
    "    \"\"\"\n",
    "    breakdown = pd.concat(\n",
    "        [series.value_counts(), series.value_counts(normalize=True)],\n",
    "        axis=1,\n",
    "        keys=(\"count\", \"fraction\")\n",
    "    )\n",
    "    display(breakdown)\n",
    "    breakdown.fraction.plot(kind=\"barh\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "show_breakdown(y_train, \"Training set class breakdown\")\n",
    "show_breakdown(y_val, \"Validation set class breakdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base collate function for basic joint embedding\n",
    "# e.g. SimCLR, MoCo, BYOL, Barlow Twins, or SimSiam\n",
    "collate_fn = WaferImageCollateFunction([input_size, input_size])\n",
    "\n",
    "# Multi crop augmentation for DINO\n",
    "dino_collate_fn = WaferDINOCOllateFunction(\n",
    "    global_crop_size=input_size, local_crop_size=input_size // 2\n",
    ")\n",
    "\n",
    "fastsiam_collate_fn = WaferFastSiamCollateFunction([input_size, input_size])\n",
    "\n",
    "# Multi crop augmentation for MSN\n",
    "msn_collate_fn = WaferMSNCollateFunction(\n",
    "    random_size=input_size, focal_size=input_size // 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightly.utils.debug import plot_augmented_images, std_of_l2_normalized\n",
    "\n",
    "# plot_augmented_images(\n",
    "#     # Grab 5 random samples from X_train to visualize original and augmented images for each\n",
    "#     X_train.sample(5).tolist(),\n",
    "#     collate_fn,\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Using cache found in C:\\Users\\khanm/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'trunc_normal_' from 'utils' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 846\u001b[0m\n\u001b[0;32m    841\u001b[0m pl\u001b[39m.\u001b[39mseed_everything(seed)\n\u001b[0;32m    842\u001b[0m dataloader_train_ssl, dataloader_train_kNN, dataloader_test \u001b[39m=\u001b[39m get_data_loaders(\n\u001b[0;32m    843\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    844\u001b[0m     model\u001b[39m=\u001b[39mBenchmarkModel,\n\u001b[0;32m    845\u001b[0m )\n\u001b[1;32m--> 846\u001b[0m benchmark_model \u001b[39m=\u001b[39m BenchmarkModel(dataloader_train_kNN, classes)\n\u001b[0;32m    848\u001b[0m \u001b[39m# Save logs to: {CWD}/benchmark_logs/cifar10/{experiment_version}/{model_name}/\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39m# If multiple runs are specified a subdirectory for each run is created.\u001b[39;00m\n\u001b[0;32m    850\u001b[0m sub_dir \u001b[39m=\u001b[39m model_name \u001b[39mif\u001b[39;00m n_runs \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m/run\u001b[39m\u001b[39m{\u001b[39;00mseed\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 520\u001b[0m, in \u001b[0;36mDINOViTModel.__init__\u001b[1;34m(self, dataloader_kNN, num_classes)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(dataloader_kNN, num_classes)\n\u001b[0;32m    516\u001b[0m \u001b[39m# self.backbone = timm.create_model(\"vit_tiny_patch16_224\", num_classes=0)\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[39m# feature_dim = (\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[39m#     timm.create_model(\"vit_tiny_patch16_224\").get_classifier().in_features\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m backbone \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mhub\u001b[39m.\u001b[39;49mload(\n\u001b[0;32m    521\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mfacebookresearch/dino:main\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdino_vits16\u001b[39;49m\u001b[39m\"\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    522\u001b[0m )\n\u001b[0;32m    523\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone \u001b[39m=\u001b[39m backbone\n\u001b[0;32m    524\u001b[0m feature_dim \u001b[39m=\u001b[39m backbone\u001b[39m.\u001b[39membed_dim\n",
      "File \u001b[1;32mc:\\Users\\khanm\\anaconda3\\envs\\timm-dev\\lib\\site-packages\\torch\\hub.py:542\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m source \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgithub\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    539\u001b[0m     repo_or_dir \u001b[39m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[39m\"\u001b[39m\u001b[39mload\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    540\u001b[0m                                        verbose\u001b[39m=\u001b[39mverbose, skip_validation\u001b[39m=\u001b[39mskip_validation)\n\u001b[1;32m--> 542\u001b[0m model \u001b[39m=\u001b[39m _load_local(repo_or_dir, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    543\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\khanm\\anaconda3\\envs\\timm-dev\\lib\\site-packages\\torch\\hub.py:569\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, hubconf_dir)\n\u001b[0;32m    568\u001b[0m hubconf_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[1;32m--> 569\u001b[0m hub_module \u001b[39m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    571\u001b[0m entry \u001b[39m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m    572\u001b[0m model \u001b[39m=\u001b[39m entry(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\khanm\\anaconda3\\envs\\timm-dev\\lib\\site-packages\\torch\\hub.py:90\u001b[0m, in \u001b[0;36m_import_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m     88\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mmodule_from_spec(spec)\n\u001b[0;32m     89\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(spec\u001b[39m.\u001b[39mloader, Loader)\n\u001b[1;32m---> 90\u001b[0m spec\u001b[39m.\u001b[39;49mloader\u001b[39m.\u001b[39;49mexec_module(module)\n\u001b[0;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m module\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_dino_main\\hubconf.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresnet\u001b[39;00m \u001b[39mimport\u001b[39;00m resnet50\n\u001b[1;32m---> 17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mvision_transformer\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mvits\u001b[39;00m\n\u001b[0;32m     19\u001b[0m dependencies \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorchvision\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdino_vits16\u001b[39m(pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\facebookresearch_dino_main\\vision_transformer.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m trunc_normal_\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop_path\u001b[39m(x, drop_prob: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m, training: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m drop_prob \u001b[39m==\u001b[39m \u001b[39m0.\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m training:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'trunc_normal_' from 'utils' (unknown location)"
     ]
    }
   ],
   "source": [
    "def get_data_loaders(batch_size: int, model):\n",
    "    \"\"\"Helper method to create dataloaders for ssl, kNN train and kNN test\n",
    "\n",
    "    Args:\n",
    "        batch_size: Desired batch size for all dataloaders\n",
    "    \"\"\"\n",
    "    col_fn = collate_fn\n",
    "    # if the model is any of the DINO models, we use the DINO collate function\n",
    "    if (\n",
    "        model == DINOModel\n",
    "        or model == DINOConvNeXtModel\n",
    "        or model == DINOXCiTModel\n",
    "        or model == DINOViTModel\n",
    "        or model == DINOSWINModel\n",
    "    ):\n",
    "        col_fn = dino_collate_fn\n",
    "    elif model == MSNModel:\n",
    "        col_fn = msn_collate_fn\n",
    "    elif model == FastSiamModel:\n",
    "        col_fn = fastsiam_collate_fn\n",
    "\n",
    "    dataloader_train_ssl = DataLoader(\n",
    "        dataset_train_ssl,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=col_fn,\n",
    "        drop_last=True,\n",
    "        # num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dataloader_train_kNN = DataLoader(\n",
    "        dataset_train_kNN,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        # num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dataloader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        # num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader_train_ssl, dataloader_train_kNN, dataloader_test\n",
    "\n",
    "\n",
    "class MocoModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        num_splits = 0 if sync_batchnorm else 8\n",
    "        # # TODO: Add split batch norm to the resnet model\n",
    "        # resnet = torchvision.models.resnet18()\n",
    "        # feature_dim = list(resnet.children())[-1].in_features\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        # )\n",
    "\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "\n",
    "        # create a moco model based on ResNet\n",
    "        self.projection_head = heads.MoCoProjectionHead(feature_dim, 2048, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # create our loss with the optional memory bank\n",
    "        self.criterion = lightly.loss.NTXentLoss(\n",
    "            temperature=0.1, memory_bank_size=memory_bank_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        return self.projection_head(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "\n",
    "        # update momentum\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "        utils.update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "        def step(x0_, x1_):\n",
    "            x1_, shuffle = utils.batch_shuffle(x1_, distributed=distributed)\n",
    "            x0_ = self.backbone(x0_).flatten(start_dim=1)\n",
    "            x0_ = self.projection_head(x0_)\n",
    "\n",
    "            x1_ = self.backbone_momentum(x1_).flatten(start_dim=1)\n",
    "            x1_ = self.projection_head_momentum(x1_)\n",
    "            x1_ = utils.batch_unshuffle(x1_, shuffle, distributed=distributed)\n",
    "            return x0_, x1_\n",
    "\n",
    "        # We use a symmetric loss (model trains faster at little compute overhead)\n",
    "        # https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb\n",
    "        loss_1 = self.criterion(*step(x0, x1))\n",
    "        loss_2 = self.criterion(*step(x1, x0))\n",
    "\n",
    "        loss = 0.5 * (loss_1 + loss_2)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.backbone.parameters()) + list(\n",
    "            self.projection_head.parameters()\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class SimCLRModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        # resnet = torchvision.models.resnet18()\n",
    "        # feature_dim = list(resnet.children())[-1].in_features\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        # )\n",
    "\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "        self.projection_head = heads.SimCLRProjectionHead(feature_dim, feature_dim, 128)\n",
    "        self.criterion = lightly.loss.NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class SimSiamModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        # resnet = torchvision.models.resnet18()\n",
    "        # feature_dim = list(resnet.children())[-1].in_features\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        # )\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "        self.projection_head = heads.SimSiamProjectionHead(feature_dim, 2048, 2048)\n",
    "        self.prediction_head = heads.SimSiamPredictionHead(2048, 512, 2048)\n",
    "        self.criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(f)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "        loss = 0.5 * (self.criterion(z0, p1) + self.criterion(z1, p0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2,  #  no lr-scaling, results in better training stability\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class FastSiamModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "        self.projection_head = heads.SimSiamProjectionHead(feature_dim, 2048, 2048)\n",
    "        self.prediction_head = heads.SimSiamPredictionHead(2048, 512, 2048)\n",
    "        self.criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(f)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Unpack augmented views\n",
    "        views, _, _ = batch\n",
    "        x1, x2, x3, x4 = views\n",
    "\n",
    "        # Pass each view through projector to get z, and predictor to get p\n",
    "        z1, p1 = self.forward(x1)\n",
    "        z2, p2 = self.forward(x2)\n",
    "        z3, p3 = self.forward(x3)\n",
    "        z4, p4 = self.forward(x4)\n",
    "\n",
    "        # Use mean of the last N - 1 projected views\n",
    "        mean = (z2 + z3 + z4) / 3\n",
    "\n",
    "        # Compute loss using prediction of 1st view, mean of remaining projected views\n",
    "        loss = self.criterion(p1, mean)\n",
    "\n",
    "        # Keep a log of the loss\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2,  #  no lr-scaling, results in better training stability\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class BarlowTwinsModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        # resnet = torchvision.models.resnet18()\n",
    "        # feature_dim = list(resnet.children())[-1].in_features\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        # )\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "        # use a 2-layer projection head for cifar10 as described in the paper\n",
    "        self.projection_head = heads.BarlowTwinsProjectionHead(feature_dim, 2048, 2048)\n",
    "\n",
    "        self.criterion = lightly.loss.BarlowTwinsLoss(\n",
    "            gather_distributed=gather_distributed\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class BYOLModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        # resnet = torchvision.models.resnet18()\n",
    "        # feature_dim = list(resnet.children())[-1].in_features\n",
    "        # self.backbone = nn.Sequential(\n",
    "        #     *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        # )\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "\n",
    "        # create a byol model based on ResNet\n",
    "        self.projection_head = heads.BYOLProjectionHead(feature_dim, 4096, 256)\n",
    "        self.prediction_head = heads.BYOLPredictionHead(256, 4096, 256)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        self.criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        return p\n",
    "\n",
    "    def forward_momentum(self, x):\n",
    "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y)\n",
    "        z = z.detach()\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, m=0.99)\n",
    "        utils.update_momentum(\n",
    "            self.projection_head, self.projection_head_momentum, m=0.99\n",
    "        )\n",
    "        (x0, x1), _, _ = batch\n",
    "        p0 = self.forward(x0)\n",
    "        z0 = self.forward_momentum(x0)\n",
    "        p1 = self.forward(x1)\n",
    "        z1 = self.forward_momentum(x1)\n",
    "        loss = 0.5 * (self.criterion(p0, z1) + self.criterion(p1, z0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = (\n",
    "            list(self.backbone.parameters())\n",
    "            + list(self.projection_head.parameters())\n",
    "            + list(self.prediction_head.parameters())\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"resnet18\").get_classifier().in_features\n",
    "\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOConvNeXtModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        self.backbone = timm.create_model(\"convnextv2_nano\", num_classes=0)\n",
    "        feature_dim = timm.create_model(\"convnextv2_nano\").get_classifier().in_features\n",
    "\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOXCiTModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        self.backbone = timm.create_model(\"xcit_tiny_12_p16_224\", num_classes=0)\n",
    "        feature_dim = (\n",
    "            timm.create_model(\"xcit_tiny_12_p16_224\").get_classifier().in_features\n",
    "        )\n",
    "\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOViTModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # self.backbone = timm.create_model(\"vit_tiny_patch16_224\", num_classes=0)\n",
    "        # feature_dim = (\n",
    "        #     timm.create_model(\"vit_tiny_patch16_224\").get_classifier().in_features\n",
    "        # )\n",
    "        self.backbone = backbone\n",
    "        feature_dim = backbone.embed_dim\n",
    "\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOSWINModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        self.backbone = timm.create_model(\"swinv2_tiny_window16_256\", num_classes=0)\n",
    "        feature_dim = (\n",
    "            timm.create_model(\"swinv2_tiny_window16_256\").get_classifier().in_features\n",
    "        )\n",
    "\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class MAEModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        decoder_dim = 512\n",
    "        vit = torchvision.models.vit_b_32(pretrained=False)\n",
    "\n",
    "        self.warmup_epochs = 40 if max_epochs >= 800 else 20\n",
    "        self.mask_ratio = 0.75\n",
    "        self.patch_size = vit.patch_size\n",
    "        self.sequence_length = vit.seq_length\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_dim))\n",
    "        self.backbone = masked_autoencoder.MAEBackbone.from_vit(vit)\n",
    "        self.decoder = masked_autoencoder.MAEDecoder(\n",
    "            seq_length=vit.seq_length,\n",
    "            num_layers=1,\n",
    "            num_heads=16,\n",
    "            embed_input_dim=vit.hidden_dim,\n",
    "            hidden_dim=decoder_dim,\n",
    "            mlp_dim=decoder_dim * 4,\n",
    "            out_dim=vit.patch_size**2 * 3,\n",
    "            dropout=0,\n",
    "            attention_dropout=0,\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward_encoder(self, images, idx_keep=None):\n",
    "        return self.backbone.encode(images, idx_keep)\n",
    "\n",
    "    def forward_decoder(self, x_encoded, idx_keep, idx_mask):\n",
    "        # build decoder input\n",
    "        batch_size = x_encoded.shape[0]\n",
    "        x_decode = self.decoder.embed(x_encoded)\n",
    "        x_masked = utils.repeat_token(\n",
    "            self.mask_token, (batch_size, self.sequence_length)\n",
    "        )\n",
    "        x_masked = utils.set_at_index(x_masked, idx_keep, x_decode)\n",
    "\n",
    "        # decoder forward pass\n",
    "        x_decoded = self.decoder.decode(x_masked)\n",
    "\n",
    "        # predict pixel values for masked tokens\n",
    "        x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
    "        x_pred = self.decoder.predict(x_pred)\n",
    "        return x_pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, _, _ = batch\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "        idx_keep, idx_mask = utils.random_token_mask(\n",
    "            size=(batch_size, self.sequence_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=images.device,\n",
    "        )\n",
    "        x_encoded = self.forward_encoder(images, idx_keep)\n",
    "        x_pred = self.forward_decoder(x_encoded, idx_keep, idx_mask)\n",
    "\n",
    "        # get image patches for masked tokens\n",
    "        patches = utils.patchify(images, self.patch_size)\n",
    "        # must adjust idx_mask for missing class token\n",
    "        target = utils.get_at_index(patches, idx_mask - 1)\n",
    "\n",
    "        loss = self.criterion(x_pred, target)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=1.5e-4 * lr_factor,\n",
    "            weight_decay=0.05,\n",
    "            betas=(0.9, 0.95),\n",
    "        )\n",
    "        cosine_with_warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optim, self.scale_lr\n",
    "        )\n",
    "        return [optim], [cosine_with_warmup_scheduler]\n",
    "\n",
    "    def scale_lr(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return epoch / self.warmup_epochs\n",
    "        else:\n",
    "            return 0.5 * (\n",
    "                1.0\n",
    "                + math.cos(\n",
    "                    math.pi\n",
    "                    * (epoch - self.warmup_epochs)\n",
    "                    / (max_epochs - self.warmup_epochs)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "class MSNModel(KNNBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        self.warmup_epochs = 15\n",
    "        #  ViT small configuration (ViT-S/16) = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)\n",
    "        #  ViT tiny configuration (ViT-T/16) = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)\n",
    "        self.mask_ratio = 0.15\n",
    "        self.backbone = masked_autoencoder.MAEBackbone(\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            num_layers=12,\n",
    "            num_heads=6,\n",
    "            hidden_dim=384,\n",
    "            mlp_dim=384 * 4,\n",
    "        )\n",
    "        self.projection_head = heads.MSNProjectionHead(384)\n",
    "\n",
    "        self.anchor_backbone = copy.deepcopy(self.backbone)\n",
    "        self.anchor_projection_head = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone)\n",
    "        utils.deactivate_requires_grad(self.projection_head)\n",
    "\n",
    "        self.prototypes = nn.Linear(256, 1024, bias=False).weight\n",
    "        self.criterion = lightly.loss.MSNLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.anchor_backbone, self.backbone, 0.996)\n",
    "        utils.update_momentum(self.anchor_projection_head, self.projection_head, 0.996)\n",
    "\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device, non_blocking=True) for view in views]\n",
    "        targets = views[0]\n",
    "        anchors = views[1]\n",
    "        anchors_focal = torch.concat(views[2:], dim=0)\n",
    "\n",
    "        targets_out = self.backbone(targets)\n",
    "        targets_out = self.projection_head(targets_out)\n",
    "        anchors_out = self.encode_masked(anchors)\n",
    "        anchors_focal_out = self.encode_masked(anchors_focal)\n",
    "        anchors_out = torch.cat([anchors_out, anchors_focal_out], dim=0)\n",
    "\n",
    "        loss = self.criterion(anchors_out, targets_out, self.prototypes.data)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def encode_masked(self, anchors):\n",
    "        batch_size, _, _, width = anchors.shape\n",
    "        seq_length = (width // self.anchor_backbone.patch_size) ** 2\n",
    "        idx_keep, _ = utils.random_token_mask(\n",
    "            size=(batch_size, seq_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=self.device,\n",
    "        )\n",
    "        out = self.anchor_backbone(anchors, idx_keep)\n",
    "        return self.anchor_projection_head(out)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = [\n",
    "            *list(self.anchor_backbone.parameters()),\n",
    "            *list(self.anchor_projection_head.parameters()),\n",
    "            self.prototypes,\n",
    "        ]\n",
    "        optim = torch.optim.AdamW(\n",
    "            params=params,\n",
    "            lr=1.5e-4 * lr_factor,\n",
    "            weight_decay=0.05,\n",
    "            betas=(0.9, 0.95),\n",
    "        )\n",
    "        cosine_with_warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optim, self.scale_lr\n",
    "        )\n",
    "        return [optim], [cosine_with_warmup_scheduler]\n",
    "\n",
    "    def scale_lr(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return epoch / self.warmup_epochs\n",
    "        else:\n",
    "            return 0.5 * (\n",
    "                1.0\n",
    "                + math.cos(\n",
    "                    math.pi\n",
    "                    * (epoch - self.warmup_epochs)\n",
    "                    / (max_epochs - self.warmup_epochs)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "models = [\n",
    "    # MSNModel,  # disabled by default because MSN uses larger images with size 224\n",
    "    # DINOModel,\n",
    "    DINOViTModel,\n",
    "    DINOConvNeXtModel,\n",
    "    DINOXCiTModel,\n",
    "    DINOSWINModel,\n",
    "    # FastSiamModel,\n",
    "    # SimSiamModel,\n",
    "    # SimCLRModel,\n",
    "    # MocoModel,\n",
    "    # BarlowTwinsModel,\n",
    "    # BYOLModel,\n",
    "    # DCL,\n",
    "    # DCLW,\n",
    "    # # MAEModel, # disabled by default because MAE uses larger images with size 224\n",
    "    # MSNModel\n",
    "    # NNCLRModel,\n",
    "    # SwaVModel,\n",
    "    # SMoGModel\n",
    "]\n",
    "bench_results = dict()\n",
    "\n",
    "experiment_version = None\n",
    "# loop through configurations and train models\n",
    "for BenchmarkModel in models:\n",
    "    runs = []\n",
    "    model_name = BenchmarkModel.__name__.replace(\"Model\", \"\")\n",
    "    for seed in range(n_runs):\n",
    "        pl.seed_everything(seed)\n",
    "        dataloader_train_ssl, dataloader_train_kNN, dataloader_test = get_data_loaders(\n",
    "            batch_size=batch_size,\n",
    "            model=BenchmarkModel,\n",
    "        )\n",
    "        benchmark_model = BenchmarkModel(dataloader_train_kNN, classes)\n",
    "\n",
    "        # Save logs to: {CWD}/benchmark_logs/cifar10/{experiment_version}/{model_name}/\n",
    "        # If multiple runs are specified a subdirectory for each run is created.\n",
    "        sub_dir = model_name if n_runs <= 1 else f\"{model_name}/run{seed}\"\n",
    "        logger = TensorBoardLogger(\n",
    "            save_dir=os.path.join(logs_root_dir, \"wafermaps\"),\n",
    "            name=\"\",\n",
    "            sub_dir=sub_dir,\n",
    "            version=experiment_version,\n",
    "        )\n",
    "        if experiment_version is None:\n",
    "            # Save results of all models under same version directory\n",
    "            experiment_version = logger.version\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=os.path.join(logger.log_dir, \"checkpoints\")\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            accelerator=\"gpu\",\n",
    "            default_root_dir=logs_root_dir,\n",
    "            strategy=distributed_backend,\n",
    "            sync_batchnorm=sync_batchnorm,\n",
    "            logger=logger,\n",
    "            callbacks=[checkpoint_callback, RichProgressBar()],\n",
    "            enable_progress_bar=True,\n",
    "        )\n",
    "        start = time.time()\n",
    "        trainer.fit(\n",
    "            benchmark_model,\n",
    "            train_dataloaders=dataloader_train_ssl,\n",
    "            val_dataloaders=dataloader_test,\n",
    "        )\n",
    "        end = time.time()\n",
    "        run = {\n",
    "            \"model\": model_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": max_epochs,\n",
    "            \"max_accuracy\": benchmark_model.max_accuracy,\n",
    "            \"max_f1\": benchmark_model.max_f1,\n",
    "            \"runtime\": end - start,\n",
    "            \"gpu_memory_usage\": torch.cuda.max_memory_allocated(),\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "        runs.append(run)\n",
    "        print(run)\n",
    "\n",
    "        # delete model and trainer + free up cuda memory\n",
    "        del benchmark_model\n",
    "        del trainer\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bench_results[model_name] = runs\n",
    "\n",
    "#  print results table\n",
    "header = (\n",
    "    f\"| {'Model':<13} | {'Batch Size':>10} | {'Epochs':>6} \"\n",
    "    f\"| {'KNN Test Accuracy':>18} | {'KNN Test F1':>18} | {'Time':>10} | {'Peak GPU Usage':>14} |\"\n",
    ")\n",
    "print(\"-\" * len(header))\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for model, results in bench_results.items():\n",
    "    runtime = np.array([result[\"runtime\"] for result in results])\n",
    "    runtime = runtime.mean() / 60  # convert to min\n",
    "    accuracy = np.array([result[\"max_accuracy\"] for result in results])\n",
    "    f1 = np.array([result[\"max_f1\"] for result in results])\n",
    "    gpu_memory_usage = np.array([result[\"gpu_memory_usage\"] for result in results])\n",
    "    gpu_memory_usage = gpu_memory_usage.max() / (1024**3)  #  convert to gbyte\n",
    "\n",
    "    if len(accuracy) > 1:\n",
    "        accuracy_msg = f\"{accuracy.mean():>8.3f} +- {accuracy.std():>4.3f}\"\n",
    "    else:\n",
    "        accuracy_msg = f\"{accuracy.mean():>18.3f}\"\n",
    "    if len(f1) > 1:\n",
    "        f1_msg = f\"{f1.mean():>8.3f} +- {f1.std():>4.3f}\"\n",
    "    else:\n",
    "        f1_msg = f\"{f1.mean():>18.3f}\"\n",
    "\n",
    "    print(\n",
    "        f\"| {model:<13} | {batch_size:>10} | {max_epochs:>6} \"\n",
    "        f\"| {accuracy_msg} | {f1_msg} | {runtime:>6.1f} Min \"\n",
    "        f\"| {gpu_memory_usage:>8.1f} GByte |\",\n",
    "        flush=True,\n",
    "    )\n",
    "print(\"-\" * len(header))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random notes:\n",
    "\n",
    "**EDIT**: Now each epoch is about a minute long for FastSiam because of the changes to DieNoise. Times below are ALL out of date.\n",
    "\n",
    "| Model              | Time per Epoch |\n",
    "| -------------------| ------------------|\n",
    "| FastSiamModel      | 20 min            |\n",
    "| SimSiamModel       | 10 min            |\n",
    "| SimCLRModel        | 12 min|\n",
    "| MocoModel          | 12 min |\n",
    "| DINOModel          |35 min |\n",
    "| BarlowTwinsModel   |                    |\n",
    "| BYOLModel          |                    |\n",
    "| DCL                |                    |\n",
    "| DCLW               |                    |\n",
    "| DINOModel          |                    |\n",
    "| MAEModel           |                    |\n",
    "| MSNModel (ViT-S)  |60 min|\n",
    "| MSNModel (ViT-T)  |50 min|\n",
    "| NNCLRModel         |                    |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51c7e371e7f0e705fbface39e2a6c77c02ee1bbdcc1060cdfeb54e5726145823"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
