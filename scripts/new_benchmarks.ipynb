{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please note this script is an adaptation of the official ImageNette benchmarking script.\n",
    "The original GitHub link is provided below.\n",
    "\n",
    "Right now, it is setup to only evaluate our FastSiamModel. You can uncomment the other models\n",
    "to allow for a comparison.\n",
    "\n",
    "Note that this benchmark also supports a multi-GPU setup. If you run it on\n",
    "a system with multiple GPUs make sure that you kill all the processes when\n",
    "killing the application. Due to the way we setup this benchmark the distributed\n",
    "processes might continue the benchmark if one of the nodes is killed.\n",
    "If you know how to fix this don't hesitate to create an issue or PR :)\n",
    "You can download the ImageNette dataset from here: https://github.com/fastai/imagenette\n",
    "\n",
    "Code has been tested on a V100 GPU with 16GBytes of video memory.\n",
    "\n",
    "Code to reproduce the benchmark results:\n",
    "\n",
    "Overall Results with Original Script (5.3.2022):\n",
    "\n",
    "\n",
    "| Model         | Batch Size | Epochs |  KNN Test Accuracy |       Time | Peak GPU Usage |\n",
    "|---------------|------------|--------|--------------------|------------|----------------|\n",
    "| BarlowTwins   |        256 |    200 |              0.587 |   86.2 Min |      4.0 GByte |\n",
    "| BYOL          |        256 |    200 |              0.619 |   88.6 Min |      4.3 GByte |\n",
    "| DCL (*)       |        256 |    200 |              0.762 |   53.3 Min |      4.3 GByte |\n",
    "| DCLW (*)      |        256 |    200 |              0.755 |   53.7 Min |      4.3 GByte |\n",
    "| DINO (Res18)  |        256 |    200 |              0.736 |   86.5 Min |      4.1 GByte |\n",
    "| MSN (ViT-S)   |        256 |    200 |              0.741 |   92.7 Min |     16.3 GByte |\n",
    "| Moco          |        256 |    200 |              0.727 |   87.3 Min |      4.3 GByte |\n",
    "| NNCLR         |        256 |    200 |              0.726 |   86.8 Min |      4.2 GByte |\n",
    "| SimCLR        |        256 |    200 |              0.771 |   82.2 Min |      3.9 GByte |\n",
    "| SimSiam       |        256 |    200 |              0.669 |   78.6 Min |      3.9 GByte |\n",
    "| SMoG          |        128 |    200 |              0.698 |  220.9 Min |     14.3 GByte |\n",
    "| SwaV          |        256 |    200 |              0.748 |   77.6 Min |      4.0 GByte |\n",
    "|---------------|------------|--------|--------------------|------------|----------------|\n",
    "| BarlowTwins   |        256 |    800 |              0.789 |  330.9 Min |      4.0 GByte |\n",
    "| BYOL          |        256 |    800 |              0.851 |  332.7 Min |      4.3 GByte |\n",
    "| DCL (*)       |        256 |    800 |              0.816 |  213.1 Min |      4.3 GByte |\n",
    "| DCLW (*)      |        256 |    800 |              0.827 |  213.1 Min |      4.3 GByte |\n",
    "| DINO (Res18)  |        256 |    800 |              0.881 |  613.9 Min |      6.7 GByte |\n",
    "| MSN (ViT-S)   |        256 |    800 |              0.834 |  376.1 Min |     16.3 GByte |\n",
    "| Moco          |        256 |    800 |              0.832 |  322.8 Min |      4.2 GByte |\n",
    "| NNCLR         |        256 |    800 |              0.848 |  341.4 Min |      4.2 GByte |\n",
    "| SimCLR        |        256 |    800 |              0.858 |  324.8 Min |      3.9 GByte |\n",
    "| SimSiam       |        256 |    800 |              0.852 |  316.0 Min |      3.9 GByte |\n",
    "| SwaV          |        256 |    800 |              0.899 |  554.7 Min |      6.6 GByte |\n",
    "\n",
    "Our FastSiam Results (12.16.2022):\n",
    "Note that this was done by a college student on consumer-grade hardware, a 3080 10GB ðŸ˜‰\n",
    "| Model         | Batch Size | Epochs |  KNN Test Accuracy |       Time | Peak GPU Usage |\n",
    "|---------------|------------|--------|--------------------|------------|----------------|\n",
    "| FastSiam      |         16 |    200 |              0.675 |  717.9 Min |      4.7 GByte |\n",
    "\n",
    "(*): Different runtime and memory requirements due to different hardware settings\n",
    "and pytorch version. Runtime and memory requirements are comparable to SimCLR\n",
    "with the default settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import lightly\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from lightly.data.collate import MultiViewCollateFunction, SimCLRCollateFunction\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.models import modules, utils\n",
    "from lightly.models.modules import heads, masked_autoencoder\n",
    "from lightly.utils import BenchmarkModule\n",
    "from lightly.utils.debug import std_of_l2_normalized\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy,\n",
    "    MulticlassAUROC,\n",
    "    MulticlassF1Score,\n",
    "    Accuracy\n",
    ")\n",
    "import warnings\n",
    "\n",
    "# suppress annoying torchmetrics and lightning warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*interpolation.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*meaningless.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*log_every_n_steps.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lightly.utils import BenchmarkModule, knn_predict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class WaferBenchmarkModule(pl.LightningModule):\n",
    "    \"\"\"A PyTorch Lightning Module for automated kNN callback\n",
    "\n",
    "    At the end of every training epoch we create a feature bank by feeding the\n",
    "    `dataloader_kNN` passed to the module through the backbone.\n",
    "    At every validation step we predict features on the validation data.\n",
    "    After all predictions on validation data (validation_epoch_end) we evaluate\n",
    "    the predictions on a kNN classifier on the validation data using the\n",
    "    feature_bank features from the train data.\n",
    "\n",
    "    We can access the highest test accuracy during a kNN prediction\n",
    "    using the `max_accuracy` attribute.\n",
    "\n",
    "    Attributes:\n",
    "        backbone:\n",
    "            The backbone model used for kNN validation. Make sure that you set the\n",
    "            backbone when inheriting from `BenchmarkModule`.\n",
    "        max_accuracy:\n",
    "            Floating point number between 0.0 and 1.0 representing the maximum\n",
    "            test accuracy the benchmarked model has achieved.\n",
    "        dataloader_kNN:\n",
    "            Dataloader to be used after each training epoch to create feature bank.\n",
    "        num_classes:\n",
    "            Number of classes. E.g. for cifar10 we have 10 classes. (default: 10)\n",
    "        knn_k:\n",
    "            Number of nearest neighbors for kNN\n",
    "        knn_t:\n",
    "            Temperature parameter for kNN\n",
    "\n",
    "    Examples:\n",
    "        >>> class SimSiamModel(BenchmarkingModule):\n",
    "        >>>     def __init__(dataloader_kNN, num_classes):\n",
    "        >>>         super().__init__(dataloader_kNN, num_classes)\n",
    "        >>>         resnet = lightly.models.ResNetGenerator('resnet-18')\n",
    "        >>>         self.backbone = nn.Sequential(\n",
    "        >>>             *list(resnet.children())[:-1],\n",
    "        >>>             nn.AdaptiveAvgPool2d(1),\n",
    "        >>>         )\n",
    "        >>>         self.resnet_simsiam =\n",
    "        >>>             lightly.models.SimSiam(self.backbone, num_ftrs=512)\n",
    "        >>>         self.criterion = lightly.loss.SymNegCosineSimilarityLoss()\n",
    "        >>>\n",
    "        >>>     def forward(self, x):\n",
    "        >>>         self.resnet_simsiam(x)\n",
    "        >>>\n",
    "        >>>     def training_step(self, batch, batch_idx):\n",
    "        >>>         (x0, x1), _, _ = batch\n",
    "        >>>         x0, x1 = self.resnet_simsiam(x0, x1)\n",
    "        >>>         loss = self.criterion(x0, x1)\n",
    "        >>>         return loss\n",
    "        >>>     def configure_optimizers(self):\n",
    "        >>>         optim = torch.optim.SGD(\n",
    "        >>>             self.resnet_simsiam.parameters(), lr=6e-2, momentum=0.9\n",
    "        >>>         )\n",
    "        >>>         return [optim]\n",
    "        >>>\n",
    "        >>> model = SimSiamModel(dataloader_train_kNN)\n",
    "        >>> trainer = pl.Trainer()\n",
    "        >>> trainer.fit(\n",
    "        >>>     model,\n",
    "        >>>     train_dataloader=dataloader_train_ssl,\n",
    "        >>>     val_dataloaders=dataloader_test\n",
    "        >>> )\n",
    "        >>> # you can get the peak accuracy using\n",
    "        >>> print(model.max_accuracy)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataloader_kNN: DataLoader,\n",
    "        num_classes: int,\n",
    "        knn_k: int = 200,\n",
    "        knn_t: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Module()\n",
    "        self.max_accuracy = 0.0\n",
    "        self.dataloader_kNN = dataloader_kNN\n",
    "        self.num_classes = num_classes\n",
    "        self.knn_k = knn_k\n",
    "        self.knn_t = knn_t\n",
    "\n",
    "        # # create a dummy metric\n",
    "        # self.dummy_accuracy = MulticlassAccuracy(num_classes=3)\n",
    "\n",
    "        # self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "        # self.val_accuracy = MulticlassAccuracy(num_classes=num_classes, average=\"macro\")\n",
    "        # self.val_auroc = MulticlassAUROC(num_classes=num_classes, average=\"macro\")\n",
    "        # self.val_f1 = MulticlassF1Score(num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "        # create dummy param to keep track of the device the model is using\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        # update feature bank at the end of each training epoch\n",
    "        self.backbone.eval()\n",
    "        self.feature_bank = []\n",
    "        self.targets_bank = []\n",
    "        with torch.no_grad():\n",
    "            for data in self.dataloader_kNN:\n",
    "                img, target, _ = data\n",
    "                img = img.to(self.dummy_param.device)\n",
    "                target = target.to(self.dummy_param.device)\n",
    "                feature = self.backbone(img).squeeze()\n",
    "                feature = F.normalize(feature, dim=1)\n",
    "                self.feature_bank.append(feature)\n",
    "                self.targets_bank.append(target)\n",
    "        self.feature_bank = torch.cat(self.feature_bank, dim=0).t().contiguous()\n",
    "        self.targets_bank = torch.cat(self.targets_bank, dim=0).t().contiguous()\n",
    "        self.backbone.train()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # we can only do kNN predictions once we have a feature bank\n",
    "        if hasattr(self, \"feature_bank\") and hasattr(self, \"targets_bank\"):\n",
    "            images, targets, _ = batch\n",
    "            feature = self.backbone(images).squeeze()\n",
    "            feature = F.normalize(feature, dim=1)\n",
    "            pred_labels = knn_predict(\n",
    "                feature,\n",
    "                self.feature_bank,\n",
    "                self.targets_bank,\n",
    "                self.num_classes,\n",
    "                self.knn_k,\n",
    "                self.knn_t,\n",
    "            )\n",
    "            num = images.size()\n",
    "            top1 = (pred_labels[:, 0] == targets).float().sum()\n",
    "\n",
    "            return (pred_labels, targets)\n",
    "\n",
    "            # print(\"pred_labels\", pred_labels)\n",
    "            # print(\"pred_labels[:, 0]\", pred_labels[:, 0])\n",
    "            # print(\"targets\", targets)\n",
    "\n",
    "            # print(\"pred_labels.shape\", pred_labels.shape)\n",
    "            # print(\"pred_labels[:, 0].shape\", pred_labels[:, 0].shape)\n",
    "            # print(\"targets.shape\", targets.shape)\n",
    "\n",
    "            # print(\"type of pred_labels\", pred_labels.type())\n",
    "            # print(\"type of pred_labels[:, 0]\", pred_labels[:, 0].type())\n",
    "            # print(\"type of targets\", targets.type())\n",
    "\n",
    "            # self.val_accuracy.to(self.dummy_param.device)\n",
    "            # self.val_auroc.to(self.dummy_param.device)\n",
    "            # self.val_f1.to(self.dummy_param.device)\n",
    "\n",
    "            # # update metrics\n",
    "            # self.val_accuracy.update(pred_labels[:, 0], targets)\n",
    "            # self.val_auroc.update(pred_labels[:, 0], targets)\n",
    "            # self.val_f1.update(pred_labels[:, 0], targets)\n",
    "\n",
    "            # print(\"metrics were updated...\")\n",
    "\n",
    "            # # log metrics\n",
    "            # self.log(\"val_acc\", self.val_accuracy, on_step=True, on_epoch=True)\n",
    "            # self.log(\"val_auroc\", self.val_auroc, on_step=True, on_epoch=True)\n",
    "            # self.log(\"val_f1\", self.val_f1, on_step=True, on_epoch=True)\n",
    "\n",
    "            # return (num, top1)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        device = self.dummy_param.device\n",
    "\n",
    "        # # This works!\n",
    "        # target = torch.tensor([2, 1, 0, 0])\n",
    "        # preds = torch.tensor([2, 1, 0, 1])\n",
    "        # metric = MulticlassAccuracy(num_classes=3)\n",
    "        # print(metric(preds, target))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if outputs:\n",
    "\n",
    "            # Initialize metrics; can't do this in __init__ because it leads to errors \n",
    "            val_acc = MulticlassAccuracy(num_classes=self.num_classes, average=\"macro\")\n",
    "            # val_auroc = MulticlassAUROC(num_classes=self.num_classes, average=\"macro\")\n",
    "            val_f1 = MulticlassF1Score(num_classes=self.num_classes, average=\"macro\")\n",
    "\n",
    "            metrics = {\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_f1\": val_f1,\n",
    "                # \"val_auroc\": val_auroc,\n",
    "            }\n",
    "\n",
    "            all_pred_labels = torch.Tensor([]).to(device)\n",
    "            all_preds = torch.Tensor([]).to(device)\n",
    "            all_targets = torch.Tensor([]).to(device)\n",
    "            for (pred_labels, targets) in outputs:\n",
    "                all_pred_labels = torch.cat((all_pred_labels, pred_labels), dim=0)\n",
    "                all_preds = torch.cat((all_preds, pred_labels[:, 0]), dim=0)\n",
    "                all_targets = torch.cat((all_targets, targets), dim=0)\n",
    "\n",
    "            results = []\n",
    "            for name, metric in metrics.items():\n",
    "                metric.to(device)\n",
    "                metric.update(all_preds, all_targets)\n",
    "                metric_val = metric.compute()\n",
    "                print(f\"{name}: {metric_val}\")\n",
    "                self.log(name, metric, metric_attribute=name)\n",
    "                print(\"well that worked\")\n",
    "                self.log(f\"{name}_val\", metric_val)\n",
    "                print(\"well that worked too\")\n",
    "                results.append(metric_val)\n",
    "\n",
    "                # self.log(f\"{name}\", metric)\n",
    "                # self.log(f\"{name}_val\", metric_val)\n",
    "\n",
    "            self.log(\"acc\", results[0])\n",
    "            self.log(\"f1\", results[-1])\n",
    "\n",
    "            # # update metrics\n",
    "            # self.accuracy(all_preds, all_targets)\n",
    "            # self.val_accuracy(all_preds, all_targets)\n",
    "            # self.val_auroc(all_preds, all_targets)\n",
    "            # self.val_f1(all_preds, all_targets)\n",
    "\n",
    "            # print(\"metrics were updated...\")\n",
    "\n",
    "            # # log metrics\n",
    "            # self.log(\"acc\", self.accuracy)\n",
    "            # self.log(\"val_acc\", self.val_accuracy)\n",
    "            # self.log(\"val_auroc\", self.val_auroc)\n",
    "            # self.log(\"val_f1\", self.val_f1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if outputs:\n",
    "        #     total_num = torch.Tensor([0]).to(device)\n",
    "        #     total_top1 = torch.Tensor([0.0]).to(device)\n",
    "        #     for (num, top1) in outputs:\n",
    "        #         total_num += num[0]\n",
    "        #         total_top1 += top1\n",
    "\n",
    "        #     if dist.is_initialized() and dist.get_world_size() > 1:\n",
    "        #         dist.all_reduce(total_num)\n",
    "        #         dist.all_reduce(total_top1)\n",
    "\n",
    "        #     acc = float(total_top1.item() / total_num.item())\n",
    "        #     if acc > self.max_accuracy:\n",
    "        #         self.max_accuracy = acc\n",
    "        #     self.log(\"kNN_accuracy\", acc * 100.0, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                     | Params\n",
      "-------------------------------------------------------------\n",
      "0 | backbone        | Sequential               | 11.2 M\n",
      "1 | projection_head | SimSiamProjectionHead    | 9.4 M \n",
      "2 | prediction_head | SimSiamPredictionHead    | 2.1 M \n",
      "3 | criterion       | NegativeCosineSimilarity | 0     \n",
      "-------------------------------------------------------------\n",
      "22.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.7 M    Total params\n",
      "90.888    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.2274688184261322\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.1813127100467682\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2168554663658142\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.19948914647102356\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.16901662945747375\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.14477740228176117\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.21768656373023987\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.1998782604932785\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.24562010169029236\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.21766319870948792\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.22360828518867493\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.19863036274909973\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2608898878097534\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.23871299624443054\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2550058364868164\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.24144193530082703\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.26843786239624023\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2394455075263977\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2847341001033783\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.265588641166687\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2973015606403351\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2803316116333008\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.28120389580726624\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.25135162472724915\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2728847563266754\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2599557340145111\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.27919042110443115\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.25969427824020386\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.24542152881622314\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.22523236274719238\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.27721738815307617\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.252769410610199\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.26053595542907715\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.244496151804924\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.28058695793151855\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.25330227613449097\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.28580111265182495\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2666935324668884\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2654755115509033\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.24053286015987396\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.27632734179496765\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2551155686378479\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2686806917190552\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.23634445667266846\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2913675904273987\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2609848380088806\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.31778672337532043\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.30424296855926514\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.29751142859458923\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.27121272683143616\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.29705601930618286\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.26552894711494446\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.27373358607292175\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.23918110132217407\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2771151661872864\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2582273483276367\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.292075514793396\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.26114422082901\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.26368969678878784\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.22375574707984924\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2935082018375397\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2804548144340515\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.296024888753891\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2734541893005371\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2917652428150177\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.27004778385162354\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.3145836889743805\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.30600884556770325\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.3370720446109772\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.3215222656726837\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.3196990191936493\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2930266857147217\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.31859952211380005\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2998155951499939\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.30914491415023804\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2970614433288574\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.2931385636329651\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2720547318458557\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.28712254762649536\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.25949913263320923\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.3145082890987396\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2882218658924103\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.28492623567581177\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.27360332012176514\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.3170021176338196\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.2999151349067688\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.24597181379795074\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.21996361017227173\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.28397876024246216\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.26143714785575867\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n",
      "val_acc: 0.24080167710781097\n",
      "well that worked\n",
      "well that worked too\n",
      "val_f1: 0.22316354513168335\n",
      "well that worked\n",
      "well that worked too\n",
      "I have now exited the loop\n",
      "everything worked...\n"
     ]
    }
   ],
   "source": [
    "logs_root_dir = os.path.join(os.getcwd(), \"benchmark_logs\")\n",
    "\n",
    "num_workers = 12\n",
    "memory_bank_size = 4096\n",
    "\n",
    "# set max_epochs to 800 for long run (takes around 10h on a single V100)\n",
    "max_epochs = 200\n",
    "knn_k = 200\n",
    "knn_t = 0.1\n",
    "classes = 10\n",
    "input_size = 128\n",
    "\n",
    "# Â Set to True to enable Distributed Data Parallel training.\n",
    "distributed = False\n",
    "\n",
    "# Set to True to enable Synchronized Batch Norm (requires distributed=True).\n",
    "# If enabled the batch norm is calculated over all gpus, otherwise the batch\n",
    "# norm is only calculated from samples on the same gpu.\n",
    "sync_batchnorm = False\n",
    "\n",
    "# Set to True to gather features from all gpus before calculating\n",
    "# the loss (requires distributed=True).\n",
    "# Â If enabled then the loss on every gpu is calculated with features from all\n",
    "# gpus, otherwise only features from the same gpu are used.\n",
    "gather_distributed = False\n",
    "\n",
    "# benchmark\n",
    "n_runs = 1  # optional, increase to create multiple runs and report mean + std\n",
    "batch_size = 32\n",
    "lr_factor = batch_size / 256  # Â scales the learning rate linearly with batch size\n",
    "\n",
    "# use a GPU if available\n",
    "gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "\n",
    "if distributed:\n",
    "    distributed_backend = \"ddp\"\n",
    "    # reduce batch size for distributed training\n",
    "    batch_size = batch_size // gpus\n",
    "else:\n",
    "    distributed_backend = None\n",
    "    # limit to single gpu if not using distributed training\n",
    "    gpus = min(gpus, 1)\n",
    "\n",
    "# The dataset structure should be like this:\n",
    "\n",
    "path_to_train = \"../data/datasets/imagenette2-160/val\"\n",
    "path_to_test = \"../data/datasets/imagenette2-160/val\"\n",
    "\n",
    "# Use SimCLR augmentations\n",
    "collate_fn = lightly.data.SimCLRCollateFunction(\n",
    "    input_size=input_size,\n",
    ")\n",
    "\n",
    "# Multi crop augmentation for SwAV\n",
    "swav_collate_fn = lightly.data.SwaVCollateFunction(\n",
    "    crop_sizes=[128, 64],\n",
    "    crop_counts=[2, 6],  # 2 crops @ 128x128px and 6 crops @ 64x64px\n",
    ")\n",
    "\n",
    "# Multi crop augmentation for DINO, additionally, disable blur for cifar10\n",
    "dino_collate_fn = lightly.data.DINOCollateFunction(\n",
    "    global_crop_size=128,\n",
    "    local_crop_size=64,\n",
    ")\n",
    "\n",
    "# Two crops for SMoG\n",
    "smog_collate_function = lightly.data.collate.SMoGCollateFunction(\n",
    "    crop_sizes=[128, 128],\n",
    "    crop_counts=[1, 1],\n",
    "    crop_min_scales=[0.2, 0.2],\n",
    "    crop_max_scales=[1.0, 1.0],\n",
    ")\n",
    "\n",
    "# Â Single crop augmentation for MAE\n",
    "mae_collate_fn = lightly.data.MAECollateFunction()\n",
    "\n",
    "# Multi crop augmentation for MSN\n",
    "msn_collate_fn = lightly.data.MSNCollateFunction(random_size=128, focal_size=64)\n",
    "\n",
    "# FastSiam collate\n",
    "base_transforms = collate_fn.transform\n",
    "fast_siam_collate_fn = MultiViewCollateFunction([base_transforms] * 4)\n",
    "\n",
    "normalize_transform = torchvision.transforms.Normalize(\n",
    "    mean=lightly.data.collate.imagenet_normalize[\"mean\"],\n",
    "    std=lightly.data.collate.imagenet_normalize[\"std\"],\n",
    ")\n",
    "\n",
    "# No additional augmentations for the test set\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(input_size),\n",
    "        torchvision.transforms.CenterCrop(128),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize_transform,\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_train_ssl = lightly.data.LightlyDataset(input_dir=path_to_train)\n",
    "\n",
    "# we use test transformations for getting the feature for kNN on train data\n",
    "dataset_train_kNN = lightly.data.LightlyDataset(\n",
    "    input_dir=path_to_train, transform=test_transforms\n",
    ")\n",
    "\n",
    "dataset_test = lightly.data.LightlyDataset(\n",
    "    input_dir=path_to_test, transform=test_transforms\n",
    ")\n",
    "\n",
    "\n",
    "def get_data_loaders(batch_size: int, model):\n",
    "    \"\"\"Helper method to create dataloaders for ssl, kNN train and kNN test\n",
    "\n",
    "    Args:\n",
    "        batch_size: Desired batch size for all dataloaders\n",
    "    \"\"\"\n",
    "    col_fn = collate_fn\n",
    "    if model == DINOModel:\n",
    "        col_fn = dino_collate_fn\n",
    "    elif model == MAEModel:\n",
    "        col_fn = mae_collate_fn\n",
    "    elif model == MSNModel:\n",
    "        col_fn = msn_collate_fn\n",
    "    elif model == FastSiamModel:\n",
    "        col_fn = fast_siam_collate_fn\n",
    "\n",
    "    dataloader_train_ssl = torch.utils.data.DataLoader(\n",
    "        dataset_train_ssl,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=col_fn,\n",
    "        drop_last=True,\n",
    "        # num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dataloader_train_kNN = torch.utils.data.DataLoader(\n",
    "        dataset_train_kNN,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        # num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        # num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader_train_ssl, dataloader_train_kNN, dataloader_test\n",
    "\n",
    "\n",
    "class MocoModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        num_splits = 0 if sync_batchnorm else 8\n",
    "        # TODO: Add split batch norm to the resnet model\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        feature_dim = list(resnet.children())[-1].in_features\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # create a moco model based on ResNet\n",
    "        self.projection_head = heads.MoCoProjectionHead(feature_dim, 2048, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # create our loss with the optional memory bank\n",
    "        self.criterion = lightly.loss.NTXentLoss(\n",
    "            temperature=0.1, memory_bank_size=memory_bank_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        return self.projection_head(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "\n",
    "        # update momentum\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "        utils.update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "        def step(x0_, x1_):\n",
    "            x1_, shuffle = utils.batch_shuffle(x1_, distributed=distributed)\n",
    "            x0_ = self.backbone(x0_).flatten(start_dim=1)\n",
    "            x0_ = self.projection_head(x0_)\n",
    "\n",
    "            x1_ = self.backbone_momentum(x1_).flatten(start_dim=1)\n",
    "            x1_ = self.projection_head_momentum(x1_)\n",
    "            x1_ = utils.batch_unshuffle(x1_, shuffle, distributed=distributed)\n",
    "            return x0_, x1_\n",
    "\n",
    "        # We use a symmetric loss (model trains faster at little compute overhead)\n",
    "        # https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb\n",
    "        loss_1 = self.criterion(*step(x0, x1))\n",
    "        loss_2 = self.criterion(*step(x1, x0))\n",
    "\n",
    "        loss = 0.5 * (loss_1 + loss_2)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.backbone.parameters()) + list(\n",
    "            self.projection_head.parameters()\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class SimCLRModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        feature_dim = list(resnet.children())[-1].in_features\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.projection_head = heads.SimCLRProjectionHead(feature_dim, feature_dim, 128)\n",
    "        self.criterion = lightly.loss.NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class SimSiamModel(WaferBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        feature_dim = list(resnet.children())[-1].in_features\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.projection_head = heads.SimSiamProjectionHead(feature_dim, 2048, 2048)\n",
    "        self.prediction_head = heads.SimSiamPredictionHead(2048, 512, 2048)\n",
    "        self.criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(f)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "        loss = 0.5 * (self.criterion(z0, p1) + self.criterion(z1, p0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2,  # Â no lr-scaling, results in better training stability\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class BarlowTwinsModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        feature_dim = list(resnet.children())[-1].in_features\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        # use a 2-layer projection head for cifar10 as described in the paper\n",
    "        self.projection_head = heads.BarlowTwinsProjectionHead(feature_dim, 2048, 2048)\n",
    "\n",
    "        self.criterion = lightly.loss.BarlowTwinsLoss(\n",
    "            gather_distributed=gather_distributed\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class BYOLModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        feature_dim = list(resnet.children())[-1].in_features\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # create a byol model based on ResNet\n",
    "        self.projection_head = heads.BYOLProjectionHead(feature_dim, 4096, 256)\n",
    "        self.prediction_head = heads.BYOLPredictionHead(256, 4096, 256)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        self.criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        return p\n",
    "\n",
    "    def forward_momentum(self, x):\n",
    "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y)\n",
    "        z = z.detach()\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, m=0.99)\n",
    "        utils.update_momentum(\n",
    "            self.projection_head, self.projection_head_momentum, m=0.99\n",
    "        )\n",
    "        (x0, x1), _, _ = batch\n",
    "        p0 = self.forward(x0)\n",
    "        z0 = self.forward_momentum(x0)\n",
    "        p1 = self.forward(x1)\n",
    "        z1 = self.forward_momentum(x1)\n",
    "        loss = 0.5 * (self.criterion(p0, z1) + self.criterion(p1, z0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = (\n",
    "            list(self.backbone.parameters())\n",
    "            + list(self.projection_head.parameters())\n",
    "            + list(self.prediction_head.parameters())\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DINOModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        feature_dim = list(resnet.children())[-1].in_features\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = heads.DINOProjectionHead(\n",
    "            feature_dim, 2048, 256, 2048, batch_norm=True\n",
    "        )\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = lightly.loss.DINOLoss(output_dim=2048)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class MAEModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        decoder_dim = 512\n",
    "        vit = torchvision.models.vit_b_32(pretrained=False)\n",
    "\n",
    "        self.warmup_epochs = 40 if max_epochs >= 800 else 20\n",
    "        self.mask_ratio = 0.75\n",
    "        self.patch_size = vit.patch_size\n",
    "        self.sequence_length = vit.seq_length\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_dim))\n",
    "        self.backbone = masked_autoencoder.MAEBackbone.from_vit(vit)\n",
    "        self.decoder = masked_autoencoder.MAEDecoder(\n",
    "            seq_length=vit.seq_length,\n",
    "            num_layers=1,\n",
    "            num_heads=16,\n",
    "            embed_input_dim=vit.hidden_dim,\n",
    "            hidden_dim=decoder_dim,\n",
    "            mlp_dim=decoder_dim * 4,\n",
    "            out_dim=vit.patch_size**2 * 3,\n",
    "            dropout=0,\n",
    "            attention_dropout=0,\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward_encoder(self, images, idx_keep=None):\n",
    "        return self.backbone.encode(images, idx_keep)\n",
    "\n",
    "    def forward_decoder(self, x_encoded, idx_keep, idx_mask):\n",
    "        # build decoder input\n",
    "        batch_size = x_encoded.shape[0]\n",
    "        x_decode = self.decoder.embed(x_encoded)\n",
    "        x_masked = utils.repeat_token(\n",
    "            self.mask_token, (batch_size, self.sequence_length)\n",
    "        )\n",
    "        x_masked = utils.set_at_index(x_masked, idx_keep, x_decode)\n",
    "\n",
    "        # decoder forward pass\n",
    "        x_decoded = self.decoder.decode(x_masked)\n",
    "\n",
    "        # predict pixel values for masked tokens\n",
    "        x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
    "        x_pred = self.decoder.predict(x_pred)\n",
    "        return x_pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, _, _ = batch\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "        idx_keep, idx_mask = utils.random_token_mask(\n",
    "            size=(batch_size, self.sequence_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=images.device,\n",
    "        )\n",
    "        x_encoded = self.forward_encoder(images, idx_keep)\n",
    "        x_pred = self.forward_decoder(x_encoded, idx_keep, idx_mask)\n",
    "\n",
    "        # get image patches for masked tokens\n",
    "        patches = utils.patchify(images, self.patch_size)\n",
    "        # must adjust idx_mask for missing class token\n",
    "        target = utils.get_at_index(patches, idx_mask - 1)\n",
    "\n",
    "        loss = self.criterion(x_pred, target)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=1.5e-4 * lr_factor,\n",
    "            weight_decay=0.05,\n",
    "            betas=(0.9, 0.95),\n",
    "        )\n",
    "        cosine_with_warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optim, self.scale_lr\n",
    "        )\n",
    "        return [optim], [cosine_with_warmup_scheduler]\n",
    "\n",
    "    def scale_lr(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return epoch / self.warmup_epochs\n",
    "        else:\n",
    "            return 0.5 * (\n",
    "                1.0\n",
    "                + math.cos(\n",
    "                    math.pi\n",
    "                    * (epoch - self.warmup_epochs)\n",
    "                    / (max_epochs - self.warmup_epochs)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "class MSNModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        self.warmup_epochs = 15\n",
    "        # Â ViT small configuration (ViT-S/16)\n",
    "        self.mask_ratio = 0.15\n",
    "        self.backbone = masked_autoencoder.MAEBackbone(\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            num_layers=12,\n",
    "            num_heads=6,\n",
    "            hidden_dim=384,\n",
    "            mlp_dim=384 * 4,\n",
    "        )\n",
    "        self.projection_head = heads.MSNProjectionHead(384)\n",
    "\n",
    "        self.anchor_backbone = copy.deepcopy(self.backbone)\n",
    "        self.anchor_projection_head = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone)\n",
    "        utils.deactivate_requires_grad(self.projection_head)\n",
    "\n",
    "        self.prototypes = nn.Linear(256, 1024, bias=False).weight\n",
    "        self.criterion = lightly.loss.MSNLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.anchor_backbone, self.backbone, 0.996)\n",
    "        utils.update_momentum(self.anchor_projection_head, self.projection_head, 0.996)\n",
    "\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device, non_blocking=True) for view in views]\n",
    "        targets = views[0]\n",
    "        anchors = views[1]\n",
    "        anchors_focal = torch.concat(views[2:], dim=0)\n",
    "\n",
    "        targets_out = self.backbone(targets)\n",
    "        targets_out = self.projection_head(targets_out)\n",
    "        anchors_out = self.encode_masked(anchors)\n",
    "        anchors_focal_out = self.encode_masked(anchors_focal)\n",
    "        anchors_out = torch.cat([anchors_out, anchors_focal_out], dim=0)\n",
    "\n",
    "        loss = self.criterion(anchors_out, targets_out, self.prototypes.data)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def encode_masked(self, anchors):\n",
    "        batch_size, _, _, width = anchors.shape\n",
    "        seq_length = (width // self.anchor_backbone.patch_size) ** 2\n",
    "        idx_keep, _ = utils.random_token_mask(\n",
    "            size=(batch_size, seq_length),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=self.device,\n",
    "        )\n",
    "        out = self.anchor_backbone(anchors, idx_keep)\n",
    "        return self.anchor_projection_head(out)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = [\n",
    "            *list(self.anchor_backbone.parameters()),\n",
    "            *list(self.anchor_projection_head.parameters()),\n",
    "            self.prototypes,\n",
    "        ]\n",
    "        optim = torch.optim.AdamW(\n",
    "            params=params,\n",
    "            lr=1.5e-4 * lr_factor,\n",
    "            weight_decay=0.05,\n",
    "            betas=(0.9, 0.95),\n",
    "        )\n",
    "        cosine_with_warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optim, self.scale_lr\n",
    "        )\n",
    "        return [optim], [cosine_with_warmup_scheduler]\n",
    "\n",
    "    def scale_lr(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return epoch / self.warmup_epochs\n",
    "        else:\n",
    "            return 0.5 * (\n",
    "                1.0\n",
    "                + math.cos(\n",
    "                    math.pi\n",
    "                    * (epoch - self.warmup_epochs)\n",
    "                    / (max_epochs - self.warmup_epochs)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "class FastSiamModel(WaferBenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        feature_dim = list(resnet.children())[-1].in_features\n",
    "        self.backbone = timm.create_model(\"resnet18\", num_classes=0)\n",
    "        self.projection_head = heads.SimSiamProjectionHead(feature_dim, 2048, 2048)\n",
    "        self.prediction_head = heads.SimSiamPredictionHead(2048, 512, 2048)\n",
    "        self.criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(f)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "        loss = 0.5 * (self.criterion(z0, p1) + self.criterion(z1, p0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Unpack augmented views\n",
    "        views, _, _ = batch\n",
    "        x1, x2, x3, x4 = views\n",
    "\n",
    "        # Pass each view through projector to get z, and predictor to get p\n",
    "        z1, p1 = self.forward(x1)\n",
    "        z2, p2 = self.forward(x2)\n",
    "        z3, p3 = self.forward(x3)\n",
    "        z4, p4 = self.forward(x4)\n",
    "\n",
    "        # Use mean of the last N - 1 projected views\n",
    "        mean = (z2 + z3 + z4) / 3\n",
    "\n",
    "        # Compute loss using prediction of 1st view, mean of remaining projected views\n",
    "        loss = self.criterion(p1, mean)\n",
    "\n",
    "        # Keep a log of the loss\n",
    "        self.log(\"loss\", loss)\n",
    "        # Monitor the STD of L2-normalized representation to check if it collapses (bad)\n",
    "        self.log(\"z1 std\", std_of_l2_normalized(z1))\n",
    "        # self.log(\"z2 std\", std_of_l2_normalized(z2))\n",
    "        # self.log(\"z3 std\", std_of_l2_normalized(z3))\n",
    "        # self.log(\"z4 std\", std_of_l2_normalized(z4))\n",
    "\n",
    "        # self.log(\"mean std\", std_of_l2_normalized(mean))\n",
    "\n",
    "        # self.log(\"p1 std\", std_of_l2_normalized(p1))\n",
    "        # self.log(\"p2 std\", std_of_l2_normalized(p2))\n",
    "        # self.log(\"p3 std\", std_of_l2_normalized(p3))\n",
    "        # self.log(\"p4 std\", std_of_l2_normalized(p4))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.parameters(), lr=0.06)\n",
    "        return optim\n",
    "\n",
    "\n",
    "models = [\n",
    "    # FastSiamModel,\n",
    "    SimSiamModel,\n",
    "    SimCLRModel,\n",
    "    # MocoModel,\n",
    "    # DINOModel\n",
    "    # BarlowTwinsModel,\n",
    "    # BYOLModel,\n",
    "    # DCL,\n",
    "    # DCLW,\n",
    "    # DINOModel,\n",
    "    # #Â MAEModel, #Â disabled by default because MAE uses larger images with size 224\n",
    "    # #Â MSNModel, #Â disabled by default because MSN uses larger images with size 224\n",
    "    # MocoModel,\n",
    "    # NNCLRModel,\n",
    "    # SimCLRModel,\n",
    "    # SimSiamModel,\n",
    "    # SwaVModel,\n",
    "    # SMoGModel\n",
    "]\n",
    "bench_results = dict()\n",
    "\n",
    "experiment_version = None\n",
    "# loop through configurations and train models\n",
    "for BenchmarkModel in models:\n",
    "    runs = []\n",
    "    model_name = BenchmarkModel.__name__.replace(\"Model\", \"\")\n",
    "    for seed in range(n_runs):\n",
    "        pl.seed_everything(seed)\n",
    "        dataloader_train_ssl, dataloader_train_kNN, dataloader_test = get_data_loaders(\n",
    "            batch_size=batch_size,\n",
    "            model=BenchmarkModel,\n",
    "        )\n",
    "        benchmark_model = BenchmarkModel(dataloader_train_kNN, classes)\n",
    "\n",
    "        # Save logs to: {CWD}/benchmark_logs/cifar10/{experiment_version}/{model_name}/\n",
    "        # If multiple runs are specified a subdirectory for each run is created.\n",
    "        sub_dir = model_name if n_runs <= 1 else f\"{model_name}/run{seed}\"\n",
    "        logger = TensorBoardLogger(\n",
    "            save_dir=os.path.join(logs_root_dir, \"imagenette\"),\n",
    "            name=\"\",\n",
    "            sub_dir=sub_dir,\n",
    "            version=experiment_version,\n",
    "        )\n",
    "        if experiment_version is None:\n",
    "            # Save results of all models under same version directory\n",
    "            experiment_version = logger.version\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=os.path.join(logger.log_dir, \"checkpoints\")\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            accelerator=\"gpu\",\n",
    "            default_root_dir=logs_root_dir,\n",
    "            strategy=distributed_backend,\n",
    "            sync_batchnorm=sync_batchnorm,\n",
    "            logger=logger,\n",
    "            callbacks=[checkpoint_callback],\n",
    "            enable_progress_bar=False,\n",
    "        )\n",
    "        start = time.time()\n",
    "        trainer.fit(\n",
    "            benchmark_model,\n",
    "            train_dataloaders=dataloader_train_ssl,\n",
    "            val_dataloaders=dataloader_test,\n",
    "        )\n",
    "        end = time.time()\n",
    "        run = {\n",
    "            \"model\": model_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": max_epochs,\n",
    "            \"max_accuracy\": benchmark_model.max_accuracy,\n",
    "            \"runtime\": end - start,\n",
    "            \"gpu_memory_usage\": torch.cuda.max_memory_allocated(),\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "        runs.append(run)\n",
    "        print(run)\n",
    "\n",
    "        # delete model and trainer + free up cuda memory\n",
    "        del benchmark_model\n",
    "        del trainer\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bench_results[model_name] = runs\n",
    "\n",
    "# Â print results table\n",
    "header = (\n",
    "    f\"| {'Model':<13} | {'Batch Size':>10} | {'Epochs':>6} \"\n",
    "    f\"| {'KNN Test Accuracy':>18} | {'Time':>10} | {'Peak GPU Usage':>14} |\"\n",
    ")\n",
    "print(\"-\" * len(header))\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for model, results in bench_results.items():\n",
    "    runtime = np.array([result[\"runtime\"] for result in results])\n",
    "    runtime = runtime.mean() / 60  # convert to min\n",
    "    accuracy = np.array([result[\"max_accuracy\"] for result in results])\n",
    "    gpu_memory_usage = np.array([result[\"gpu_memory_usage\"] for result in results])\n",
    "    gpu_memory_usage = gpu_memory_usage.max() / (1024**3)  # Â convert to gbyte\n",
    "\n",
    "    if len(accuracy) > 1:\n",
    "        accuracy_msg = f\"{accuracy.mean():>8.3f} +- {accuracy.std():>4.3f}\"\n",
    "    else:\n",
    "        accuracy_msg = f\"{accuracy.mean():>18.3f}\"\n",
    "\n",
    "    print(\n",
    "        f\"| {model:<13} | {batch_size:>10} | {max_epochs:>6} \"\n",
    "        f\"| {accuracy_msg} | {runtime:>6.1f} Min \"\n",
    "        f\"| {gpu_memory_usage:>8.1f} GByte |\",\n",
    "        flush=True,\n",
    "    )\n",
    "print(\"-\" * len(header))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51c7e371e7f0e705fbface39e2a6c77c02ee1bbdcc1060cdfeb54e5726145823"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
